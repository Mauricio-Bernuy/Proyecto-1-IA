{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAU07kSOoutX",
        "outputId": "d310e41d-07fa-4c0d-f58b-c6d5e7a27c63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: colorama in c:\\users\\mauri\\appdata\\roaming\\python\\python310\\site-packages (0.4.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install colorama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gSjG79QLkzXB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math\n",
        "import csv\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from colorama import init, Fore, Back, Style\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdkwUsS5_pB6"
      },
      "source": [
        "# Leer Dataframe \n",
        "### **(hay muchas areas con valor 0, afectará los resultados?)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7ZOTer3trw7O"
      },
      "outputs": [],
      "source": [
        "def ReadFile():\n",
        "    dataset = pd.read_csv('./forestfires.csv', header=0)\n",
        "    return dataset\n",
        "dataset = ReadFile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYqzn6LX-_uc"
      },
      "source": [
        "## Convertir dia y mes a INT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "iv-byBAb-702",
        "outputId": "f9c48797-41ce-435f-ff9f-e6cfaf1a1fca"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>FFMC</th>\n",
              "      <th>DMC</th>\n",
              "      <th>DC</th>\n",
              "      <th>ISI</th>\n",
              "      <th>temp</th>\n",
              "      <th>RH</th>\n",
              "      <th>wind</th>\n",
              "      <th>rain</th>\n",
              "      <th>area</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>86.2</td>\n",
              "      <td>26.2</td>\n",
              "      <td>94.3</td>\n",
              "      <td>5.1</td>\n",
              "      <td>8.2</td>\n",
              "      <td>51</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>90.6</td>\n",
              "      <td>35.4</td>\n",
              "      <td>669.1</td>\n",
              "      <td>6.7</td>\n",
              "      <td>18.0</td>\n",
              "      <td>33</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>90.6</td>\n",
              "      <td>43.7</td>\n",
              "      <td>686.9</td>\n",
              "      <td>6.7</td>\n",
              "      <td>14.6</td>\n",
              "      <td>33</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>91.7</td>\n",
              "      <td>33.3</td>\n",
              "      <td>77.5</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.3</td>\n",
              "      <td>97</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>89.3</td>\n",
              "      <td>51.3</td>\n",
              "      <td>102.2</td>\n",
              "      <td>9.6</td>\n",
              "      <td>11.4</td>\n",
              "      <td>99</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   X  Y  month  day  FFMC   DMC     DC  ISI  temp  RH  wind  rain  area\n",
              "0  7  5      3    4  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0   0.0\n",
              "1  7  4     10    1  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0   0.0\n",
              "2  7  4     10    5  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0   0.0\n",
              "3  8  6      3    4  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2   0.0\n",
              "4  8  6      3    6  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0   0.0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from calendar import month_abbr, day_abbr\n",
        "\n",
        "month_mapping = [m.lower() for m in month_abbr]\n",
        "day_mapping = [d.lower() for d in day_abbr]\n",
        "\n",
        "dataset['month'] = dataset['month'].str.lower().map(lambda m: month_mapping.index(m)).astype('Int8') \n",
        "dataset['day'] = dataset['day'].str.lower().map(lambda d: day_mapping.index(d)).astype('Int8')\n",
        "\n",
        "dataset.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OheWsbjfAA2b"
      },
      "source": [
        "## Dividir en train (70%), test (15%), validation (15%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RbItVyC1UMN",
        "outputId": "84f5e482-6721-4c47-852f-aa331594c96e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(361, 13) (78, 13) (78, 13)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(dataset, test_size=0.3)\n",
        "test, validation = train_test_split(test, test_size=0.5)\n",
        "# not needed\n",
        "\n",
        "# train = train.round(4)\n",
        "# test = test.round(4)\n",
        "# validation = validation.round(4) \n",
        "\n",
        "print(train.shape, test.shape, validation.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z6arq-6_NGE"
      },
      "source": [
        "## Normalizar columnas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "krhKQ_y7_Mpj"
      },
      "outputs": [],
      "source": [
        "def MinMax(DatasetInput):\n",
        "  max = DatasetInput.max()\n",
        "  min = DatasetInput.min()\n",
        "  return (DatasetInput - min) / (max - min), min, max\n",
        "\n",
        "def ApplyMinMax(DatasetInput, min, max):\n",
        "  return (DatasetInput - min) / (max - min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wuYJs7FmQKh"
      },
      "source": [
        "#### se normaliza usando los datos **MIN, MAX** de **training**, estos luego se usan para normalizar el dataset de **test** y **validation** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7DNKaCXQZieL"
      },
      "outputs": [],
      "source": [
        "train, train_mean, train_std = MinMax(train)\n",
        "test = ApplyMinMax(test, train_mean, train_std)\n",
        "validation = ApplyMinMax(validation, train_mean, train_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rFfUIRDaf3A",
        "outputId": "66c397cb-fc2e-43c7-b6d8-32035389566e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "X        1.000000\n",
              "Y        1.000000\n",
              "month    1.000000\n",
              "day      1.000000\n",
              "FFMC     0.998710\n",
              "DMC      0.995520\n",
              "DC       0.993784\n",
              "ISI      0.361854\n",
              "temp     1.006472\n",
              "RH       0.988235\n",
              "wind     0.900000\n",
              "rain     0.031250\n",
              "area     0.684133\n",
              "dtype: float64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD_Rnsx5aMyJ"
      },
      "source": [
        "## Separar variables en X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GEkrtVQ6Uc-5"
      },
      "outputs": [],
      "source": [
        "# X_train = train[[\"DMC\",\"temp\"]].to_numpy()\n",
        "X_train = train[[\"X\",\"Y\",\"month\",\"day\",\"FFMC\",\"DMC\",\"DC\",\"ISI\",\"temp\",\"RH\",\"wind\",\"rain\"]].to_numpy()\n",
        "Y_train = train[\"area\"].to_numpy()\n",
        "\n",
        "# X_test = test[[\"DMC\",\"temp\"]].to_numpy()\n",
        "X_test = test[[\"X\",\"Y\",\"month\",\"day\",\"FFMC\",\"DMC\",\"DC\",\"ISI\",\"temp\",\"RH\",\"wind\",\"rain\"]].to_numpy()\n",
        "Y_test = test[\"area\"].to_numpy()\n",
        "\n",
        "# X_validation = validation[[\"DMC\",\"temp\"]].to_numpy()\n",
        "X_validation = validation[[\"X\",\"Y\",\"month\",\"day\",\"FFMC\",\"DMC\",\"DC\",\"ISI\",\"temp\",\"RH\",\"wind\",\"rain\"]].to_numpy()\n",
        "Y_validation = validation[\"area\"].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "zQ4_lpxSxclj",
        "outputId": "a66e93bb-de0f-4216-ef68-2df6f4b3aaf6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x289e084b0a0>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.scatter(train[[\"month\"]].to_numpy(),Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmhvAea8jqeI"
      },
      "source": [
        "# Hipótesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "esFnu9BiaKoT"
      },
      "outputs": [],
      "source": [
        "def h(X, W):    \n",
        "    result = np.einsum(\"ijk,ik->j\", X.tolist(), W)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya2VOHaXgEkW"
      },
      "source": [
        "# Termino Regularizador y derivadas **(ridge, lasso, elasticnet, None)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "bdwPyf8Nf5mZ"
      },
      "outputs": [],
      "source": [
        "def Regularizador(W, lambd, regularizer = \"ridge\", ratio = 0.3):\n",
        "    if regularizer == \"ridge\":\n",
        "        return lambd * np.sum(np.power((W),2)) # l2 norm \n",
        "\n",
        "    elif regularizer == \"lasso\":\n",
        "        return lambd * np.sum(np.abs((W))) # l1 norm\n",
        "\n",
        "    elif regularizer == \"elasticnet\": # sum of proportions of l1 and l2 norm\n",
        "        return ((ratio) * (lambd * np.sum(np.abs((W))) )) + ((1-ratio) * (lambd * np.sum(np.power((W),2)))) \n",
        "    \n",
        "    elif regularizer == \"None\":\n",
        "        return 0\n",
        "\n",
        "def Reg_Derivadas(W, lambd, regularizer = \"ridge\", ratio = 0.3):\n",
        "\n",
        "    C = 0.0000001\n",
        "\n",
        "    if regularizer == \"ridge\":\n",
        "        return 2 * lambd * (W) # l2 derivative\n",
        "\n",
        "    elif regularizer == \"lasso\":\n",
        "        return lambd * ( W / (np.power((np.power(W, 2) + C), 1/2))) # l1 derivative\n",
        "\n",
        "    elif regularizer == \"elasticnet\": # sum of proportions of l1 and l2 derivatives\n",
        "        return ((ratio) * (lambd * ( W / np.power((np.power(W, 2) + C), 1/2))) + ((1-ratio) * (2 * lambd * (W)))) \n",
        "    \n",
        "    elif regularizer == \"None\":\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuqwOassjuCn"
      },
      "source": [
        "# Función Loss **(ridge, lasso, elasticnet, None)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "wM968NlPbhia"
      },
      "outputs": [],
      "source": [
        "def Error(X, W, Y, lambd, regularizer = \"ridge\", ratio = 0.3):\n",
        "\n",
        "  features_len = W.shape[0]\n",
        "  y_pred = h(X, W)\n",
        "  l2 = np.sum(np.power((Y - y_pred),2)) # l2 norm\n",
        "  l2 = l2 / (2*len(Y))\n",
        "  reg = Regularizador(W, lambd, regularizer, ratio)\n",
        "  l2 = l2 + reg\n",
        "  \n",
        "  return l2 + reg \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3NO2rUyjwxO"
      },
      "source": [
        "# Derivadas **(ridge, lasso, elasticnet, None)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "A5SGz1QfeJri"
      },
      "outputs": [],
      "source": [
        "def derivada(X, W, Y, lambd, regularizer = \"ridge\", ratio = 0.3):   \n",
        "    \n",
        "    A = Y - h(X,W)\n",
        "    dw = np.einsum(\"ijk,j->ik\", (-X).tolist(), A) \n",
        "    dw = dw / len(A)\n",
        "\n",
        "    reg_deriv = (Reg_Derivadas(W, lambd, regularizer, ratio))\n",
        "    dw = dw + reg_deriv\n",
        "\n",
        "    return dw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2ey7zRIkJ8u"
      },
      "source": [
        "# Update de Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "IA6q2ZRLd21D"
      },
      "outputs": [],
      "source": [
        "def update(W,  dW, alpha):\n",
        "    W = W - (alpha * dW)\n",
        "    return W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vi0-h6DkQlO"
      },
      "source": [
        "### Función generica para mostrar barra de progreso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "ZXAYnToZd2tj"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "def print_progress_bar(index, total, label):\n",
        "    width = 50 \n",
        "    sys.stdout.write('\\r')\n",
        "    sys.stdout.write(f\"[{'=' * int(width * index / total):{width}s}] {round((100 * index / total),5)}%  {label}\")\n",
        "    sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjP0IAdN5idS"
      },
      "source": [
        "## Calcular matrices potenciadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "2pgbW9Nv5hm6"
      },
      "outputs": [],
      "source": [
        "def getPowMatrices(X, X_val, X_train, p_degree):\n",
        "    features_len = 0\n",
        "    try:\n",
        "        features_len = X.shape[1]\n",
        "    except IndexError:\n",
        "        features_len = 1\n",
        "\n",
        "    pows_3d = np.array([])\n",
        "    pows_3d_val = np.array([])\n",
        "    pows_3d_train = np.array([])\n",
        "\n",
        "\n",
        "    for j in range (0, features_len):\n",
        "        pows = np.array(np.ones(len(X)))\n",
        "        pows_val = np.array(np.ones(len(X_val)))\n",
        "        pows_train = np.array(np.ones(len(X_train)))\n",
        "\n",
        "        for i in range(1,p_degree+1):\n",
        "            pows = np.column_stack((pows,np.power(X.T[j],i)))\n",
        "            pows_val = np.column_stack((pows_val,np.power(X_val.T[j],i)))\n",
        "            pows_train = np.column_stack((pows_train,np.power(X_train.T[j],i)))\n",
        "\n",
        "            \n",
        "        if j == 0:\n",
        "            pows_3d = [pows]\n",
        "            pows_3d_val = [pows_val]\n",
        "            pows_3d_train = [pows_train]\n",
        "\n",
        "\n",
        "        else:\n",
        "            pows_3d = np.row_stack((pows_3d, [pows]))\n",
        "            pows_3d_val = np.row_stack((pows_3d_val, [pows_val]))\n",
        "            pows_3d_train = np.row_stack((pows_3d_train, [pows_train]))\n",
        "\n",
        "    return pows_3d, pows_3d_val, pows_3d_train, features_len\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bFgFqtcjDeD"
      },
      "source": [
        "# Entrenamiento (por numero de de epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "MP8BjeeRgS4W"
      },
      "outputs": [],
      "source": [
        "# 3D\n",
        "\n",
        "def train_epochs_3d(X, Y, X_val, Y_val, X_train, Y_train, epochs, alfa, p_degree, lambd, regularizer = \"ridge\", ratio = 0.3):\n",
        "    np.random.seed(2001)\n",
        "\n",
        "    # pre process x values\n",
        "    X, X_val, X_train, features_len = getPowMatrices(X, X_val, X_train, p_degree)\n",
        "\n",
        "    W = np.random.rand(features_len, p_degree + 1)\n",
        "\n",
        "    loss_train = []\n",
        "    loss_val = []\n",
        "\n",
        "    for e in range(epochs):\n",
        "        L_train = 0\n",
        "        dW = derivada(X, W, Y, lambd, regularizer, ratio)\n",
        "        W = update(W, dW, alfa)\n",
        "        L_train = Error(X, W, Y, lambd, regularizer, ratio) \n",
        "        L_val = Error(X_val, W, Y_val, lambd, regularizer, ratio) \n",
        "        loss_train.append(L_train)\n",
        "        loss_val.append(L_val)\n",
        "        print_progress_bar(e, epochs, f\"{Style.BRIGHT}TRAINING IN PROGRESS {Style.RESET_ALL}| {Fore.RED}{Style.BRIGHT}last computed training loss:{Style.RESET_ALL} \"+str(L_train) + \" | \"  + f\"{Fore.RED}{Style.BRIGHT}• R^2 training:{Style.RESET_ALL}\" + str(rsquared(Y, h(X, W))) + f\"{Fore.RED}{Style.BRIGHT}• R^2 validation:{Style.RESET_ALL}\" + str(rsquared(Y_validation, h(X_val, W))))\n",
        "\n",
        "    print(f\"\\nTraining Completed!\\n-------------------------------\\n\")\n",
        "    return W, loss_train, loss_val, X, X_val, X_train\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LL-vAKjkwIz"
      },
      "source": [
        "# Graficar Loss por Epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "x_n-NSmelN3z"
      },
      "outputs": [],
      "source": [
        "def Plot_Loss(epochs, loss, axes1, axes2, label=\"loss\"):    \n",
        "\n",
        "    # main fig\n",
        "    axes1.plot(epochs, loss, label=label)\n",
        "    axes1.set_xlabel('epochs')\n",
        "    axes1.set_ylabel('loss')\n",
        "    axes1.set_title(\"Loss over Iteration\")\n",
        "    axes1.legend(loc=\"upper right\")\n",
        "\n",
        "    # zoom\n",
        "    axes2.plot(epochs[-9:], loss[-9:], label=label)\n",
        "    axes2.set_xlabel('epochs')\n",
        "    axes2.set_ylabel('loss')\n",
        "    axes2.set_title('zoomed in last values');\n",
        "    axes2.annotate('%0.4f' % loss[-1], xy=(1, loss[-1]), xytext=(8, 0), \n",
        "                 xycoords=('axes fraction', 'data'), textcoords='offset points')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SclwheJBirWm"
      },
      "source": [
        "# R^2 (Coeficiente de determinación)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "4PIUzLDcDC3N"
      },
      "outputs": [],
      "source": [
        "def rsquared(Y, Y_estimation):\n",
        "  return 1 - (np.sum(np.power(Y - Y_estimation, 2) / np.sum(np.power(Y - np.mean(Y),2))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8TczdJi3Bsz"
      },
      "source": [
        "# Entrenar y mostrar resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "A9eI5WW0r9QJ"
      },
      "outputs": [],
      "source": [
        "def train_output_print(X_train, Y_train, X_validation, Y_validation, X_test, Y_test, epochs, alfa, p_degree, lambd, regularizer = \"ridge\", ratio = 0.3):\n",
        "    W, loss_train, loss_val, X_train_pow, X_val_pow, X_test_pow = train_epochs_3d(X_train, Y_train, X_validation, Y_validation, X_test, Y_test, epochs, alfa, p_degree, lambd, regularizer, ratio)\n",
        "\n",
        "\n",
        "    print((f\"{Fore.GREEN}{Style.BRIGHT}• with epochs:{Style.RESET_ALL}\"), epochs, \"->\", f\"{Fore.RED}{Style.BRIGHT}\\tlast training loss:{Style.RESET_ALL}\",loss_train[-1],f\"{Fore.RED}{Style.BRIGHT}\\n\\t\\t\\tlast validation loss:{Style.RESET_ALL}\",loss_val[-1])\n",
        "    print(f\"{Fore.GREEN}{Style.BRIGHT}\\t\\t\\tmin training loss:{Style.RESET_ALL}\", min(loss_train))\n",
        "    # print(f\"{Fore.GREEN}{Style.BRIGHT}• W array:\\n{Style.RESET_ALL}\", W)\n",
        "\n",
        "    fig = plt.figure()\n",
        "    axes1 = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # main axes\n",
        "    axes2 = fig.add_axes([0.5, 0.5, 0.4, 0.3]) # zoom axes\n",
        "\n",
        "    Plot_Loss(range(len(loss_train)),loss_train, axes1, axes2, \"train\")\n",
        "    Plot_Loss(range(len(loss_val)),loss_val, axes1, axes2, \"validation\")\n",
        "    r2_train = rsquared(Y_train, h(X_train_pow, W))\n",
        "    r2_val = rsquared(Y_validation, h(X_val_pow, W))\n",
        "\n",
        "    print(f\"{Fore.RED}{Style.BRIGHT}• R^2 training:{Style.RESET_ALL}\",r2_train)\n",
        "    print(f\"{Fore.RED}{Style.BRIGHT}• R^2 validation:{Style.RESET_ALL}\",r2_val)\n",
        "\n",
        "    return W, loss_train, loss_val, X_test_pow, r2_train, r2_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "bkSxgSor41yM"
      },
      "outputs": [],
      "source": [
        "def test_output_print(X_test, model_W, Y_test, lambd, regularizer, ratio):\n",
        "    loss_test = Error(X_test, model_W, Y_test, lambd, regularizer, ratio)\n",
        "    print(f\"{Fore.GREEN}{Style.BRIGHT}• test loss on current model:\\n{Style.RESET_ALL}\", loss_test)\n",
        "    r2_test = rsquared(Y_test, h(X_test, model_W))\n",
        "    print(f\"{Fore.RED}{Style.BRIGHT}• R^2 test:{Style.RESET_ALL}\",r2_test)\n",
        "    return loss_test, r2_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prueba hiperparámetros ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaCUnzcHXdW_",
        "outputId": "f142f131-26cd-4375-9ad0-3bb731215218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.01721751436441252 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0166294954092469\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.01721751436441252\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -6.484120170202477\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -99.94153861382466\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.07147832191253066\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -18.456585536772398\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.020167933297146185 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.016885842770205755\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.020167933297146185\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -6.974880872626694\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -83.93316372552356\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.0244065664218949\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -295.82606409226673\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.03708463043875403 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0389058932457244\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.03708463043875403\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -15.52056484832151\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -248.3990234677576\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.5831513205797546\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -166.30808018921138\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.738775358545232 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.7259814945135663\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.738775358545232\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1035.8784105040895\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -14568.97839472985\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 76692.69994372563\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -22373118.62806\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 355846884.92120475 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 356516513.51166916\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 92.1562649289727\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -214140505772.2688\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -3037273701614.632\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 8158787701.477839\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2380096678685.236\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 832305153652064.4 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 832990853570723.1\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 209.4570770789089\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -5.008656687930558e+17\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -7.096552928007372e+18\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 5.532490583188112e+18\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.613961905755794e+21\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 4.948407169929838e+30 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 4.9458820916879834e+30\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 392.4319242620335\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -2.977911765579351e+33\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -4.213651234799392e+34\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 3.6531007815384297e+47\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.0656982736875082e+50\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0289748641835249 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.03157629040681529\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0289748641835249\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -12.797869736952041\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -216.50180212478492\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.09677165768160642\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -25.465497906649635\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.08962614307985305 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.08988218524880429\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.08962614307985305\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -47.271970773332725\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -684.5636590288458\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.360772294706005\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -393.2205916014914\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.2837908922845433 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.2885530663259169\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.2837908922845433\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -160.05242191640573\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2319.582830459834\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 15.056072623170694\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4386.492155482963\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.4754342645071266 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.4404114550395903\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.4754342645071266\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -269.91276789129614\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -3535.860273163844\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 118445.6959252848\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -34553479.51549205\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.8234556838265715 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.1038524841737667\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.8234556838265715\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -470.42607952096034\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -9062.181619248633\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 50933739.3201212\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -14858609511.597446\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.8406745681458837 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.0914819729527305\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.8406745681458837\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -478.1057675188639\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -8918.772721388428\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 58233555630.88068\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -16988143338418.572\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.5320444345297206 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 4.757605699329406\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.5320444345297206\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1467.2799872508022\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -39749.18776476095\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 8.121934075035172e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.3693655446110168e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.9341027356539047 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.9821743254146071\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.9341027356539047\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1159.0582013123917\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -16831.454641651402\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 2.1771591567876327\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -632.1603026559773\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.0814585818632696 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.2366720129370177\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.0814585818632696\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1245.4322827172723\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -18967.194232290556\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.082001979879362\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1186.725591454303\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.1523717680008496 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.284069953913964\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.1523717680008496\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1283.0352698623467\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -19299.164271033456\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 22.45104314170007\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -6542.959538773705\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.6107866305286533 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.7522096073308258\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.6107866305286533\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1552.1626596334788\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -23192.040852616195\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 125124.75634979634\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -36501922.29659754\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.5815919391085713 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 3.515823120150116\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.5815919391085713\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1523.9068170533576\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -29547.498839357584\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 54138312.77085257\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -15793461460.723843\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.368786515813486 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 4.500114308147479\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.368786515813486\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1994.93530508567\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -37895.255641054326\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 62214884694.11285\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -18149593778302.973\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 7.908655382191544 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 12.755376499739533\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 7.908655382191544\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -4686.74434640892\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -107661.60776794559\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 8.927588697678899e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.6043945766264447e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 28.309724911764192 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 28.544198258030562\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 28.309724911764192\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -17030.06497223137\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -243104.1853819165\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 29.42633238988211\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -8579.326233967697\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 39.22621748717535 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 40.049505802658466\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 39.22621748717535\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -23597.106520217734\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -341090.45923781843\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 44.44333756306742\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -12958.446814589328\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 49.9898978844393 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 50.945110133901345\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 49.9898978844393\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -30069.668842131214\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -433846.0568059398\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 80.90175850953233\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -23591.336445610857\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 63.56960062328755 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 65.27609850945002\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 63.56960062328755\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -38235.0198237981\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -555843.8564940041\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 127543.1940307907\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -37207435.550858796\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 52.84205132090469 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 57.309913880743714\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 52.84205132090469\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -31768.8750589193\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -487832.65858920984\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 54863995.17690497\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -16005160650.594494\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 102.2295626040177 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 107.70993392433074\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 102.2295626040177\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -61486.884802249995\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -917175.372623057\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 62980019506.59526\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -18372802196997.836\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 132.0045826086153 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 149.7850692728184\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 132.0045826086153\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -79363.835952614\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1275066.3383317003\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 9.060688147014746e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.643222920509757e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.04252056494216208 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0416791997366094\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.04252056494216208\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -4.522323578085041\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -70.0098122006264\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.08062383144650812\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -12.792198261533692\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.057965827541463386 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.055226098234950456\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.057965827541463386\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -4.856623426216203\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -58.56661299121592\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.7582424094929484\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -206.12627398603797\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.08676986191050035 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.08762238480875584\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.08676986191050035\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -10.788992459455713\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -173.16000232469545\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.461419336120238\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -114.00822296164714\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.5695665461835815 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.563127146022184\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.5695665461835815\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1476.5974921676052\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -20862.34472487109\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 53517.7520791115\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -15612394.395612579\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 497393768.81865066 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 498327681.1033822\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 93.04025405394867\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -298802992734.7986\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -4238084843158.71\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 10985703818.65408\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -3204518198184.517\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1137457303759450.2 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1138392844525269.5\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 210.91656124664217\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -6.833605521902454e+17\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -9.682245419244714e+18\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 7.547976073635152e+18\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.2019274335566168e+21\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 6.423925603872038e+30 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 6.420652533298725e+30\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 395.0692490424976\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -3.8600448921842375e+33\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -5.461841789381853e+34\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.73524204961151e+47\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.3813851792061214e+50\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.08053514042693935 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.08299957831392721\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.08053514042693935\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -12.342614803811857\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -208.8895292939132\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.145946416775268\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -24.548932473504692\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.16832111015924212 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.1685369109970203\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.16832111015924212\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -45.5843203469919\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -660.3288500078445\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.3944966422128566\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -379.28360176986666\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.4145317663082426 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.4190979395168377\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.4145317663082426\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -154.36395579185861\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2237.381616475661\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 14.663656046523604\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4231.117338512724\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.6785257727106889 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.6447146170904613\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.6785257727106889\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -260.3181233082507\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -3410.354187987015\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 114255.99984839739\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -33331179.457054954\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.1434220805843165 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.413851890492704\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.1434220805843165\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -453.723659708472\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -8740.798811790144\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 49131936.43210799\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -14332979721.271011\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.1987535887054692 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.440640188203768\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.1987535887054692\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -461.1343291867678\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -8602.495926710886\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 56173484239.8965\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -16387170450796.26\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.246208839506665 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 5.392704180847513\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.246208839506665\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1415.2071034899498\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -38338.28042840568\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 7.834566372186561e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.285533402232963e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.987589549935701 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.035496770344605\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.987589549935701\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1154.8180206909706\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -16770.026205008446\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 2.229743383202925\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -629.8418730329413\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.168940332121158 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.323602775356916\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.168940332121158\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1240.8715896314802\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -18897.933975557626\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.162243298556094\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1182.4029134193975\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.3152761878997787 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.446507308991086\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.3152761878997787\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1278.3410976687733\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -19228.72946583038\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 22.540892379015833\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -6519.372404072275\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.873188287340682 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 3.014103484702163\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.873188287340682\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1546.4960204694871\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -23107.491993243577\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 124675.35850998036\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -36370742.73025976\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.003313826660408 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 3.934170456608445\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.003313826660408\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1518.3639151016207\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -29440.274119113405\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 53943754.42275817\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -15736703936.818134\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.8290029955362272 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 4.956249002641553\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.8290029955362272\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1987.6572141369131\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -37757.43970496278\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 61991301137.35363\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -18084369021249.832\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 8.959038174874859 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 13.788261311285426\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 8.959038174874859\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -4669.796080064558\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -107272.57339191982\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 8.895504922643654e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.595034959765823e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 28.424408668641185 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 28.658802594593816\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 28.424408668641185\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -17023.917350516447\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -243016.4773195746\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 29.540604463930453\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -8576.226518294465\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 39.38953594016807 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 40.21253387977563\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 39.38953594016807\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -23588.588645451222\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -340967.3981764253\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 44.60476485835889\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -12953.766690329183\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 50.23931342001685 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 51.19418869260374\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 50.23931342001685\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -30058.814050897083\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -433689.51514434203\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 81.1400284067621\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -23582.823912148622\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 63.93631009598518 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 65.64220108118027\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 63.93631009598518\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -38221.21749010107\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -555643.2867317263\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 127497.67533132309\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -37194042.97229883\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 53.351783631739494 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 57.818043262852704\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 53.351783631739494\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -31757.405044041378\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -487656.6201585756\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 54844247.96206967\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -15999399753.880161\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 102.84075634298276 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 108.3191633054207\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 102.84075634298276\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -61464.68961702208\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -916844.4192055655\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 62957350518.0671\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -18366189101965.41\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 133.26900213491305 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 151.04309130405798\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 133.26900213491305\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -79335.1873667838\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1274606.2501950772\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 9.057426842238526e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.6422715186520387e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.010574373151994412 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.009093725614289151\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.010574373151994412\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.11527214375218242\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2.172294136923914\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.013087989339733548\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.27383094194895463\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.014491716080059532 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.012948637819113384\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.014491716080059532\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.11748456117269135\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1.6716465057974244\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.03425704021237616\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -5.3076457015503875\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.019677772073971162 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.01826302230048702\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.019677772073971162\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.26047865249584934\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -4.789480673453901\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.028883082594271536\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.296339215661025\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 93.60628877683607 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 93.93870984898152\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 93.60628877683607\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -55205.46133954367\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -784384.437073764\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1744.4297276279742\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -508341.67891042284\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 13737206216.180655 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 13762530759.884853\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 101.79604643742064\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -8112282747098.539\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -115060644487696.27\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 273954028358.81302\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -79843416689721.05\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.517621942811852e+16 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.519658726185283e+16\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 225.50080279971428\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.487757397910819e+19\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2.1079402670047373e+20\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.6431155113104287e+20\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4.793353879655905e+22\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 8.572945070019832e+31 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 8.568641854014634e+31\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 421.25275352542025\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -5.074930890969018e+34\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -7.180867164172991e+35\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 6.225582039839294e+48\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.8161535718055573e+51\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.422968063938569 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.4242938486332339\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.422968063938569\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -8.587120259474203\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -146.0209328189397\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.46873131018818115\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -16.99691855032205\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.6883431310933377 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.6882070077670228\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.6883431310933377\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -31.668828277898896\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -460.329757628182\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.543391176746666\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -264.27216039061955\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.27072716748505 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.2736514640072407\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.27072716748505\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -107.44210606699534\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1559.122882682727\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 11.202745245239678\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2948.9705529242856\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.006501867168796 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.9826613614522193\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.006501867168796\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -181.18424796796074\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2375.022387059776\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 79674.7637411269\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -23242566.75239485\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.231318770468153 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 3.419485752825977\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.231318770468153\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -315.94494755200736\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -6089.321152463552\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 34260332.222629115\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -9994570695.679585\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.539936377685466 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 3.7082074242072895\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.539936377685466\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -321.1324329554955\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -5993.2277833572425\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 39170269024.237564\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -11426919368542.56\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 7.866697958177944 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 9.361010382817382\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 7.866697958177944\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -985.672600830728\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -26700.18583461355\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 5.462791267723912e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.5936289666437514e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.500277043759798 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.546571275894573\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.500277043759798\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1113.2543967948568\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -16167.86954771912\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 2.733584687452617\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -607.1162451143226\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.008248458452007 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 3.157507433702176\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.008248458452007\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1196.167083658915\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -18219.01367286237\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.930563727319373\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1140.0267806425793\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.87942118625781 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 4.006071817505968\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.87942118625781\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1232.327565685434\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -18538.2899496363\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 23.388719080278427\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -6288.103196655005\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 5.393275028969483 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 5.5292115069631205\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 5.393275028969483\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1490.9480528132578\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -22278.681039770712\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 120268.7746757416\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -35084473.04592573\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 7.054648310301795 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 7.952419265101245\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 7.054648310301795\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1464.0250824179993\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -28389.083016422417\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 52036029.234742604\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -15180172930.741291\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 8.24974866253263 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 9.336974583342856\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 8.24974866253263\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1916.3122403332404\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -36406.40647449467\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 59798971803.525185\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -17444813276426.1\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 19.04863916432003 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 23.70630107711633\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 19.04863916432003\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -4503.630784024988\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -103458.32289387376\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 8.580910307682755e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.5032600654706734e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 29.566506706223315 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 29.800107937597872\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 29.566506706223315\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -16962.5623614526\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -242141.1257655794\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 30.67859388118088\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -8545.29051262353\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 41.01597891337643 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 41.836078745945464\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 41.01597891337643\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -23503.577875743715\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -339739.2137692548\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 46.21233361869474\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -12907.057762997532\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 52.72324544177787 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 53.67475750649775\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 52.72324544177787\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -29950.480227218846\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -432127.1852671875\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 83.51272359639823\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -23497.86639008395\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 67.58844600328034 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 69.28827984922404\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 67.58844600328034\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -38083.46639209172\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -553641.5443185638\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 127043.37625839078\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -37060380.71084916\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 58.4285725027524 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 62.87883434458627\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 58.4285725027524\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -31642.93115745426\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -485899.707418167\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 54647164.36335641\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -15941904142.209005\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 108.92778893302032 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 114.3865909004793\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 108.92778893302032\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -61243.17557736495\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -913541.4118745852\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 62731106683.11255\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -18300188275368.688\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 145.8622541055137 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 163.57249423672326\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 145.8622541055137\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -79049.26665851964\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1270014.4425014534\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 9.024877966237503e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.6327762205370632e+26\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016742826214563309 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013724227229181972\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016742826066954747\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0018918768410388642\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.03309373632142876\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003466774723181114\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.006674683453771646\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016721884578719767 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013813091139957363\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016715528205445084\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.00340592136279505\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.03707583580895135\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.0034623875325899118\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.005271927486653727\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 769073.8194765262 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 771818.4187477083\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 107.66447136663338\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -380475947.0437918\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -5409750267.670463\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 793013.7841572231\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -191392238.9174034\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.6289329905941904e+16 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.6358332177217716e+16\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 175.29512067182796\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.3163892677563187e+19\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1.869480062723401e+20\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 2.961870109658808e+16\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -7.351509351885204e+18\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.3723671586985867e+23 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.376105395101922e+23\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 183.22279227141755\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.1975555489495052e+26\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1.6985539241124014e+27\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.0673393521505074e+24\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.1753769143468194e+27\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 6.627911872228652e+28 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 6.632519188654145e+28\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 373.2117416906867\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -3.365389528195111e+31\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -4.768277482814718e+32\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 3.7168974826813446e+32\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.0842790825381196e+35\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.843605866587048e+42 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.8423629215586006e+42\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 669.7828300922861\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.4658478944639756e+45\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2.074128542668413e+46\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.7982030732970446e+59\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -5.245795354556858e+61\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.1088438802811032 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.10743874688384722\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.1088438802811032\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.20796779805225674\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -4.12802983345238\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.11146103624467907\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.34896356697696596\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.16932749510187664 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.1679698611202865\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.16932749510187664\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.7366516539996342\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -12.017299325770779\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.19300402697336236\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -6.74873679494773\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.29195015180525197 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.2906900708700334\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.29195015180525197\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -2.6871689116803865\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -40.46184022971214\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.5521380716962071\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -76.69022930831326\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.4556267730746779 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.45363166525895426\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.4556267730746779\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -4.565875963244981\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -60.795404541037605\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 2089.008763280638\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -609283.4024853896\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.7233352644905047 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.7268523880739877\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.7233352644905047\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -8.058357348877912\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -157.20755740970594\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 897999.4589134733\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -261968057.24633494\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.8022680571843984 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.8053346208824135\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.8022680571843984\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -8.207911220470331\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -155.48558305046643\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1026703951.9140265\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -299514492956.2586\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.6724164342394376 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.709335770282745\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.6724164342394376\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -25.507158961164784\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -688.8470123387511\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.4310115010255839e+22\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4.174608305297563e+24\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 5.807783731312347 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 5.840655736789392\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 5.807783731312347\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -771.2168045717051\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -11211.304164258827\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 5.968407716061744\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -420.13451539957657\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 8.484496995723013 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 8.58908277590136\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 8.484496995723013\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -828.3516542708921\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -12631.226056628293\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 9.821493123854973\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -791.0028692199177\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 14.183085751900673 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 14.271862519558473\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 14.183085751900673\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -853.6901244527629\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -12855.232802363797\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 27.781202561558068\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4380.15073756339\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 22.042755979300395 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 22.137610232039226\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 22.042755979300395\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1033.6906538903254\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -15455.269972962295\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 83891.97518092224\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -24467397.942787632\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 33.9124325539305 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 34.5373539113974\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 33.9124325539305\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1016.4208501922374\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -19727.454427300992\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 36289326.56725572\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -10586469042.627733\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 37.52508259726489 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 38.28224277635005\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 37.52508259726489\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1328.9729603902224\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -25279.02591576407\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 41703099613.08178\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -12165807593384.814\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 85.85166961234364 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 89.09431600602898\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 85.85166961234364\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -3133.6060741423476\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -72005.74021257671\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 5.984197385873774e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.745735802242305e+26\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 40.52297926743623 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 40.748802472736934\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 40.52297926743623\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -16360.976635564386\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -233558.2547636939\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 41.5947902372657\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -8241.965521050453\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 56.619682978062684 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 57.411359566037305\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 56.619682978062684\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -22670.048128516377\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -327696.8154406654\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 61.63098304258615\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -12449.079189228452\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 76.56055165372312 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 77.47907919848541\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 76.56055165372312\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -28888.27061947034\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -416808.5204028342\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 106.25932960820779\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -22664.852871311297\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 102.6436073863246 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 104.28404141333841\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 102.6436073863246\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -36732.82292148364\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -534014.4640080786\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 122588.02795527418\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -35749766.269507684\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 107.18361599387514 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 111.4770086032923\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 107.18361599387514\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -30520.52228501071\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -468673.1924353244\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 52714675.7952892\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -15378135497.556133\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 167.35703828673593 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 172.62359829982216\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 167.35703828673593\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -59071.243232776804\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -881155.4787726783\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 60512690696.013664\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -17653022403068.297\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 266.801120367454 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 283.8852968063903\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 266.801120367454\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -76245.83457653373\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1224991.8543116536\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 8.705722555364874e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.539670831240425e+26\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.1 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.5436496641700527e+36 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.546616128643652e+36\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 197.5298300184213\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -6.528489202147377e+38\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -9.267600380961007e+39\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 2.5630884666579038e+36\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -3.220923639314385e+38\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.1 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.409253734732715e+45 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.4138415084549012e+45\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 326.51302084275943\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -6.547203581340311e+47\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -9.307909242323721e+48\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 2.438057743874523e+45\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -3.2573146732902956e+47\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.1 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.772449050073645e+55 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.7782172125603797e+55\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 509.4150341740111\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -8.010554567435108e+57\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1.1389620065888931e+59\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 2.822867854669606e+55\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4.029624501647252e+57\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.1 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.999536978010884e+61 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 4.005814413894189e+61\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 733.427490857901\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.1975764250975136e+64\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1.70074711565339e+65\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.3024243191166703e+61\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -6.68798698480501e+63\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.1 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 5.963862172491311e+65 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 5.96957392202066e+65\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 660.6287779657496\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.829776559083087e+68\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2.595265127705962e+69\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 6.448500575450019e+66\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.7958892410847583e+69\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.1 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.868519973484318e+69 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.8693190001964136e+69\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1392.5917631791867\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -5.836447687323888e+71\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -8.26941483413301e+72\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 6.44677161039108e+72\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.8804177319370285e+75\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.1 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.3084943275187136e+78 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.3081310062938106e+78\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2104.623721861175\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -4.284772378093554e+80\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -6.062817787442312e+81\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 5.2562690083772965e+94\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.5333814047982098e+97\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.01 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0017023090258213053 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001500485302345693\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0017023090258213053\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.005033449527706679\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.0014449594811056787\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003518005826692627\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.016800612326852482\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.01 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016999880728950794 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014997100555085466\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016999880728950794\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.0035489446945926595\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m 0.00045491775534889367\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003512773221749792\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.015231705651900018\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.01 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016973521852149077 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014958292734496784\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016973521852149077\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.002103240671208262\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m 0.0017681632009303172\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.0035077069962494425\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.013822015928532938\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.01 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016958170340347196 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014922315660369188\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016958170340347196\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.0014068921093191022\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m 0.0016109574610719823\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003553493927526231\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.02728951245954625\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.01 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.001694796950674258 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014899275460799802\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.001694796950674258\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.0010020958524137313\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m 0.0006127917938678618\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.026265278165077014\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -6.652970291470768\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.01 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016940161807565551 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001488539876617775\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016940161807565551\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.0007427501323911123\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.001186144861086369\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 31.829435699618017\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -9284.410748036695\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.01 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.001692162466915882 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014859924653866057\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.001692162466915882\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.00020808495326596343\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.007241693274231409\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 441425370491935.5\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.2877450785713443e+17\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.001 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 4.766972045759086 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 4.773913456595184\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 4.766972045759086\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -148.89980089783978\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2180.265332344527\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.796852332404047\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -80.36996071347043\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.001 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 7.365625462083078 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 7.386900284841966\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 7.365625462083078\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -159.662785833598\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2454.770123658459\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 7.629072085704175\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -153.7235206102617\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.001 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 13.029087207704041 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 13.047146667339984\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 13.029087207704041\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -164.88573313531998\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2501.312992314089\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 15.740285315722373\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -870.3235608875223\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.001 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 20.64534730016196 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 20.664205379713845\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 20.64534730016196\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -200.5400763821925\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -3012.873309953409\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 16789.225227352505\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4891898.934190282\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.001 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 32.537504400742506 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 32.66100903237748\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 32.537504400742506\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -198.50806780021816\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -3875.793637855645\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 7255675.093648738\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2116647352.0918934\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.001 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 35.72663836419747 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 35.87693158753236\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 35.72663836419747\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -258.35821765720243\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -4951.350872688844\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 8338093678.111103\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2432424543526.379\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.001 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 81.62471323312786 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 82.26687862219889\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 81.62471323312786\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -619.6159223917899\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -14256.858245759353\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.1964513339246726e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -3.490339296299611e+25\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.0001 epochs: 100 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 79.97214599517653 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 80.16647336108593\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 79.97214599517653\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -13933.84435618486\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -198929.1876519907\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 80.88163990429615\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -7018.235759064627\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.0001 epochs: 100 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 112.81687820232983 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 113.4937312563639\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 112.81687820232983\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -19307.14154560984\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -279110.04535405413\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 117.08170933580215\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -10601.388792767772\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.0001 epochs: 100 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 162.55300660937297 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 163.33828830327639\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 162.55300660937297\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -24602.79391098059\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -355004.0327117919\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 187.850839448204\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -19303.91340077127\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.0001 epochs: 100 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 229.24604098502599 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 230.6466287724933\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 229.24604098502599\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -31283.69139506048\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -454827.8841471396\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 104594.02934031522\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -30460905.511561606\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.0001 epochs: 100 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 283.77111245743095 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 287.4313865601758\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 283.77111245743095\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -25992.247650490415\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -399172.0500599758\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 44916273.98995574\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -13103098468.706556\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.0001 epochs: 100 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 378.42849672908585 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 382.91914241096276\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 378.42849672908585\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -50308.70361345381\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -750493.4736816236\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 51560473643.61275\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -15041443071042.56\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.0001 epochs: 100 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 100 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 704.8199340638403 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 719.3777400181123\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 704.8199340638403\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -64935.619481424364\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1043347.1003397637\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 7.417800005366878e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.1639525249970203e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.00412276132093107 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.002317970352071647\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.00412276132093107\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.48477521589240924\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -4.641308163650405\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.01849976278678288\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -3.913758782741751\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.006011379905305401 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0040214409253414435\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.006011379905305401\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.659178892525369\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -5.532689668380832\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.32506574874872124\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -92.88008060175166\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.006983605699855568 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.005357438472845836\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.006983605699855568\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.44321299528414526\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -5.574958250219385\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 46.92603548177755\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -13687.127127876129\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.009524125914119178 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.007572632191999144\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.009524125914119178\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.5361129123526718\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -4.118045527044066\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 3120.995130716096\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -910467.0904953933\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.103203387314419e+68 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 3.10903269516553e+68\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 92.1562649289727\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.8674367296447571e+71\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2.6486804624237367e+72\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 6.282881985308732e+69\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.8328519482488605e+72\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.052307772547958e+129 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 3.054822435042388e+129\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 209.4570770789089\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.8368217079414303e+132\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2.6025146618375683e+133\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 2.0286174056244477e+133\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -5.917969790976774e+135\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.6530567541709533e+286 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.652213230689726e+286\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 392.4319242620335\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -9.947963028042193e+288\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1.4076053958802807e+290\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.220348834120486e+303\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -3.5600541118142274e+305\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.01723208469343687 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.01665208450504956\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.01723208469343687\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -6.492163607211879\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -100.12373627990101\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.07151034805360235\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -18.465576299345035\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.020247910208349967 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.016966370350862653\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.020247910208349967\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -7.021346267341487\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -84.59566235241292\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.0248947439741383\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -295.96766689968405\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.03730832568423411 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.03915472327590859\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.03730832568423411\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -15.650677589378471\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -250.45518987686938\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.5924584102737231\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -169.02098988948927\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.05881473786879516 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.04959086490762772\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.05881473786879516\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -24.92344040202273\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -287.3997142782559\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 76493.7377234219\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -22315076.610898953\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.09766851094799933 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.1005566116950615\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.09766851094799933\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -44.199918927126014\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -663.5005187324566\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 31174019.35606047\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -9094219013.02036\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.11341260036303105 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.09689797185368336\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.11341260036303105\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -51.64444437555069\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -603.5624471417639\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 32248548389.42427\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -9407685252262.094\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.19008384936520425 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.15862901892150907\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.19008384936520425\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -84.05498864839012\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -935.0909935653423\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 3.58715778517126e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.0464610991355709e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.028975989903661528 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.031577946375912776\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.028975989903661528\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -12.798520788978522\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -216.51553723072308\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.09677334103959997\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -25.46597613384168\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.08964716231827541 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.08990473498437765\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.08964716231827541\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -47.28435044224621\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -684.7519581643428\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.3608077463181314\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -393.2308020303894\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.28395217935422745 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.2886706408934255\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.28395217935422745\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -160.14820611905\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2320.566367048305\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 15.057583462162608\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4386.9322770828485\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.4760620871744144 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.4407971516599721\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.4760620871744144\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -270.2874948997782\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -3539.102107430644\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 118447.9389509249\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -34554133.85905093\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.8242376193800386 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.104491218922286\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.8242376193800386\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -470.89205900623756\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -9067.55824293625\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 50934921.67943306\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -14858954434.539528\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.8413764293982485 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.092415278386806\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.8413764293982485\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -478.52362305193753\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -8926.660387145235\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 58234735297.61976\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -16988487475873.996\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.5364505380750777 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 4.761705084520508\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.5364505380750777\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1469.912957080679\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -39783.84895170144\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 8.122230523272952e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.3694520257661047e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.972775482450379 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.021352632935192\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.972775482450379\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1182.3287537190922\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -17165.201608185063\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 2.217934477381285\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -644.0524023182904\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.130591098901745 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.287814226405427\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.130591098901745\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1274.9975643873797\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -19402.87176790731\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.1403580355970035\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1203.7459672215068\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.206856228411132 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.340920634092525\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.206856228411132\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1315.821907756212\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -19783.48417128489\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 22.537293554349514\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -6568.117405295925\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.6729341999557925 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.8192938328734116\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.6729341999557925\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1589.5611557190612\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -23763.552954736402\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 125130.83142778161\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -36503694.54075599\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.628527875051435 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 3.574602191501302\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.628527875051435\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1552.1516428357504\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -30048.272971612532\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 54139714.292870946\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -15793870318.75294\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.4548246419565083 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 4.600450775607934\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.4548246419565083\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -2046.7112930047865\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -38750.07898350355\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 62216128618.28985\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -18149956661214.59\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 7.995311567892501 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 12.883442421926196\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 7.995311567892501\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -4738.892208618039\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -108752.71035012287\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 8.927757688158376e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.6044438752560248e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0021120535330990636 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0006120461697629827\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0021120535330990636\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0006421504170288639\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.36634195660836855\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.004176569224368682\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.0866362901609794\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0025475218102435286 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0010221687473849783\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0025475218102435286\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.005484598028494325\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.23710852476798072\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.01248382269860186\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.3859968290604647\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.002901812743412512 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.001418419805704836\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.002901812743412512\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.009050070993398163\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.38888235435164176\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.3509468339005348\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -392.73778390189295\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0035525812689868722 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00206564985690735\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0035525812689868722\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.006462831700346339\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.39535852985998776\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 77.42856220882051\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -22586.238395507284\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 8.697934191932776e+69 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 8.714244862487929e+69\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 93.04025405394867\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -5.225173563963529e+72\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -7.411129336775953e+73\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.7581294660134795e+71\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -5.128403760415923e+73\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 6.833359323409851e+130 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 6.838979652691086e+130\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 210.91656124664217\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -4.105339325819234e+133\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -5.816680922851768e+134\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.53400831812146e+134\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.3226800405759382e+137\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.2168900607465416e+287 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.2157605274876684e+287\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 395.0692490424976\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.3320974872997847e+290\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1.8848759345768932e+291\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.6341271182639793e+304\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4.7671459208592476e+306\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.04256891817737366 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0417351609331616\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.04256891817737366\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -4.533309270726178\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -70.23016266476705\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.08070801184660636\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -12.80797444113362\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.058085052951133975 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.05534662268096391\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.058085052951133975\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -4.899944420048463\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -59.19097340979245\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.7590940350066578\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -206.3609296330001\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.08707093313347433 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0879499379896932\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.08707093313347433\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -10.909957577991268\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -175.0981458528214\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.4724096972014389\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -117.185177725998\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.14038457248940872 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.13352919002576272\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.14038457248940872\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -17.38493770916697\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -200.8595834602149\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 53352.028958207935\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -15564050.336934252\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.21035603729048602 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.21197763402484468\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.21035603729048602\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -30.8346411167026\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -463.4976773831573\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 21743260.258393086\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -6343037363.586218\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.24254180136330822 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.2307092234823997\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.24254180136330822\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -36.035128862837276\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -422.4782605126718\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 22493640973.36659\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -6561941700379.735\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.43930142648300197 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.4168835462087079\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.43930142648300197\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -58.62779878957632\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -652.1238950616325\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 2.5019691270225645e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -7.298851958758477e+25\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.08053750662442458 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.08300258493420441\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.08053750662442458\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -12.343496570667295\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -208.907469393804\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.14594980728661944\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -24.549658663861074\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.16835067980857674 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.1685681969974073\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.16835067980857674\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -45.59870290820188\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -660.5470875594874\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.394550266121668\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -379.29758989476056\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.414729085945937 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.419251427819666\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.414729085945937\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -154.46789417619425\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2238.4795785126425\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 14.665759914477524\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4231.723901650093\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.6792297832449925 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.6451761431455769\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.6792297832449925\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -260.7075058680533\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -3413.800445391619\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 114259.5468089223\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -33332214.17498464\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.144323628894361 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.4146304869106392\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.144323628894361\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -454.21457956364543\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -8746.701053875075\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 49133737.419920206\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -14333505113.157412\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.1995761786136694 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.4417148311602295\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.1995761786136694\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -461.5782340020472\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -8610.92797678161\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 56175437135.19256\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -16387740157812.31\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.2510469721221726 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 5.397486772130235\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.2510469721221726\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1417.915212695947\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -38376.145621766074\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 7.83500751075203e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.2856620930728254e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.026309944278286 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.074722281355879\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.026309944278286\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1178.0606461581597\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -17103.37402860628\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 2.270563639502035\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -641.7196460086567\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.218110250639807 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.374779769795376\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.218110250639807\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1270.3944132200072\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -19332.9881067242\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.2206235845992985\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1199.3988683372017\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.369776652394149 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.5033703422457063\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.369776652394149\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1311.0729276325715\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -19712.242345126215\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 22.627113868091033\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -6544.490599899779\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.935339237994569 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 3.08118243964672\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.935339237994569\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1583.8274808422552\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -23677.981355116503\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 124681.43835523019\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -36372516.33166799\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.0502455574447573 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 3.9929237201780214\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.0502455574447573\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1546.5559926627034\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -29940.117424837805\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 53945160.77035684\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -15737114202.561928\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.9150191334542788 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 5.056537065221849\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.9150191334542788\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -2039.332336544761\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -38610.6100086531\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 61992551794.62219\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -18084733868367.824\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 9.045674368451767 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 13.916222063254661\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 9.045674368451767\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -4721.842738957894\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -108361.51745021336\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 8.895675293604953e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.5950846611154983e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016588776757859947 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001405577862325286\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016588661827657348\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.00911458766953055\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.09035843682969014\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003434045793551598\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m 0.001879833632543515\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016555601368602245 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001445757679636311\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016555455823536935\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.012441081563786316\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.1057711149886722\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.0034280164094036554\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m 0.004283223620864307\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016527149091246785 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.000146260783522182\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.001652713207892611\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.014338740760272528\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.1175080690503787\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003493991088739456\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.014873461788718956\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 82.40828402634045 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 82.7204074911675\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 82.40828402634045\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -48615.95799588086\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -690924.4775539235\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 93.82430228427398\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -26892.906167497753\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.9247501043960974e+84 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.9282981574852075e+84\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 101.79604643742064\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.136629738309646e+87\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1.612139749143343e+88\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 3.827720624720078e+85\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.1155794449296953e+88\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.661829637271036e+144 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.663174075349454e+144\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 225.50080279971428\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -9.820375708041582e+146\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1.3914073235542718e+148\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.0845821349353448e+148\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -3.1639808330316986e+150\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.4719978425063466e+298 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 3.4702550625673804e+298\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 421.25275352542025\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -2.0553204249414928e+301\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2.9082135832786348e+302\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m inf\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -inf\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 1000 p_degree: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-79-4bddb9028ed6>:12: RuntimeWarning: overflow encountered in power\n",
            "  l2 = np.sum(np.power((Y - y_pred),2)) # l2 norm\n",
            "<ipython-input-86-197391f0b7ea>:2: RuntimeWarning: overflow encountered in power\n",
            "  return 1 - (np.sum(np.power(Y - Y_estimation, 2) / np.sum(np.power(Y - np.mean(Y),2))))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.010940506664432186 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.009462015816272012\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.010940506664432186\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.120881743564627\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2.2700857641105014\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.013488104478255089\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.286463058419826\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.015013170920494404 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.013470777542125074\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.015013170920494404\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.12530402793950568\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1.7881818371599643\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.03551866249144926\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -5.527360455075228\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.020451161997435158 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.019042900598177703\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.020451161997435158\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.2806888974550896\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -5.130882355315692\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.031297952309215206\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.784994224908446\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.032317996834477655 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.030688752324821815\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.032317996834477655\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.4613923062886971\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -5.806099767603652\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1444.9389343750352\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -421514.0816547078\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.045873816171554054 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.044503321719178396\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.045873816171554054\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.8308286294240519\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -13.240970939803521\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 589129.644785425\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -171863420.60413685\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0525076185013018 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.05083701496235214\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0525076185013018\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.9790047599527179\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -12.781474684297951\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 610212022.2284551\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -178013675916.781\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.09683780612465565 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.09476078238541553\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.09683780612465565\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.5863169874187681\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -17.91605583553477\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 6.783850717685716e+21\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.9790141118819225e+24\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.4231469730509605 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.4244749688457923\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.4231469730509605\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -8.592276227527355\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -146.11276605200564\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.4689287962504959\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -17.00483688300849\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.6887406385290906 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.6886077389961746\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.6887406385290906\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -31.70613826940125\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -460.88542157356966\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.5441508735162048\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -264.3959031578739\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.2718341353229894 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.274717568654823\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.2718341353229894\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -107.62980429337425\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1561.4319166419973\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 11.212601463849204\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2951.6138952190854\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.008774586537702 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.9846954241722736\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.008774586537702\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -181.70901203038653\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2380.417844915416\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 79712.50735383606\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -23253577.072781514\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.2347884435834438 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 3.423036656370319\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.2347884435834438\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -316.6782526054262\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -6100.394645838586\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 34277212.96593383\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -9999495218.071651\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.5434596787687593 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 3.712162032705877\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.5434596787687593\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -321.83048180355934\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -6006.785143805933\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 39189912948.77434\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -11432649978705.459\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 7.879039427726646 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 9.375472741985337\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 7.879039427726646\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -989.05333898142\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -26766.118619072702\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 5.465938825097938e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.5945471856200204e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.539460853070656 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.5862557743972037\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.539460853070656\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1136.2159008312137\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -16497.199834287025\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 2.7748415154692316\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -618.8499338524763\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.0577795805776873 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 3.2090195786460503\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.0577795805776873\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1225.2649794233175\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -18647.830142141593\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.989178306284484\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1156.7797594068643\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.934077276480439 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 4.063054204521049\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.934077276480439\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1264.5136205464114\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -19013.767268640368\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 23.47470482063672\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -6312.842693790452\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 5.4554656991606105 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 5.59624409487245\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 5.4554656991606105\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1527.6138104689162\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -22839.012569528564\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 120275.2820098174\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -35086371.02197439\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 7.101562706811313 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 8.010942270733905\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 7.101562706811313\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1491.6950925150647\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -28879.721149768116\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 52037647.936746895\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -15180645145.177757\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 8.335563062242173 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 9.436800635150956\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 8.335563062242173\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1966.9878063694478\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -37243.208381103716\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 59800478165.016396\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -17445252718679.396\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 19.135184449737302 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 23.8333384795202\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 19.135184449737302\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -4554.6826806891295\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -104526.09100732233\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 8.581121360560344e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.503321634723961e+26\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016742826214747107 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013724227228306557\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016742826066954747\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0018918768521011264\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.03309373593361986\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003466774723561778\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.006674683554097394\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016721877161872227 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013812766165668836\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016715528205445084\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0034054751818412\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.03706078133247925\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003462390664259378\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.0052732736923579715\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.475523015251477e+41 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.4843416765226625e+41\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 107.66447136663338\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.2246944757020163e+44\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1.741303259045805e+45\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 2.552605857569526e+41\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -6.160695648192848e+43\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 5.762540198598695e+145 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 5.777665283876969e+145\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 175.29512067182796\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -2.8854847574027613e+148\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -4.09784275606041e+149\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 6.492328116643121e+145\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.611428222694977e+148\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.1052364783828982e+216 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.1069780449117574e+216\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 183.22279227141755\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -5.5791620312048896e+218\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -7.913209178208587e+219\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.894888742443306e+217\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -5.475836389078188e+219\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 6.778084630356904e+267 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 6.782796337641619e+267\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 373.2117416906867\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -3.4416412704282284e+270\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -4.87631533771069e+271\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 3.80111356120118e+271\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.108846274067592e+274\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 1000 p_degree: 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mauri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "<ipython-input-78-3c36073289a9>:3: RuntimeWarning: overflow encountered in power\n",
            "  return lambd * np.sum(np.power((W),2)) # l2 norm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m inf \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m inf\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 669.7828300922861\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -inf\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -inf\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m inf\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -inf\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016742826213720155 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013724227247349685\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016742826175260878\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.001891876790281688\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.03309373930650095\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.0034667747201737368\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.0066746826256440794\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016721877162258138 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013812766167303422\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016721877162258138\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0034054752050568515\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.037060780814264005\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003462390660791155\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.0052732726580826395\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016700450295408732 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013851426183510564\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016700450295408732\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.00472607174727302\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.03991712330847097\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003460154730532359\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.004606006981025645\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016689496386479842 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013842517141093898\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016689496386479842\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.005157357013945041\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.04238608545846567\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.0036675905778209964\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.06523060223793786\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.001668294491019099 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001384129051200401\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.001668294490950493\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.005325744581062164\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.045480192994659996\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.07511986249199798\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -20.909704223435977\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.001667871228582412 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013856514250601236\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.001667871211148771\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.00543253425947432\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.04887218224643819\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 27.175671581901813\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -7926.799295690444\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.001667074444654606 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014022962732009286\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016670731316433837\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0059508092712632354\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.06250733992777335\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 364571817638895.4\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.0635445883572614e+17\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.11289077815834372 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.11148966103677552\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.11289077815834372\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.21649775088954115\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -4.283010062385474\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.11554619121979545\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.3642583737085183\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.1757947458296506 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.17444170654283672\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.1757947458296506\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.7707966712802163\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -12.539838388732349\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.2003086367563626\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -7.009564123485497\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.3035486910945423 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.3022916974781116\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.3035486910945423\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -2.8142379089734617\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -42.28705143560776\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.5745934378504511\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -79.91901710545446\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.47392709190310434 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.4718899707683633\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.47392709190310434\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -4.802027503913554\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -63.78058752186723\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 2168.9076423983556\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -632586.6229938638\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.7525873587460427 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.756318290609127\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.7525873587460427\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -8.453810727270708\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -164.6278033682768\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 932489.5047785984\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -272029633.44206816\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.8344860161694334 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.8377831242067895\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.8344860161694334\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -8.606608709020321\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -163.09436259271095\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1066259192.0069656\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -311053717732.57587\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.7423647881714635 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.7812082456401164\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.7423647881714635\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -26.851682774418347\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -724.2766434593048\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.4870609122635234e+22\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4.3381180587085233e+24\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 5.851611751237787 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 5.884940709166479\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 5.851611751237787\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -791.2485895957147\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -11498.786004630829\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 6.014059279002601\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -430.37545528824626\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 8.538483291845452 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 8.644802063760881\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 8.538483291845452\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -853.2263144139847\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -12998.141101288906\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 9.883627067412732\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -805.4359397022569\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 14.242609154771763 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 14.333371563431268\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 14.242609154771763\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -880.6739294212161\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -13254.159630115744\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 27.8720418847379\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4402.3648126326825\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 22.11179134698795 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 22.21067402613174\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 22.11179134698795\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1064.1535572340422\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -15920.856540443941\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 83927.95918498971\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -24477889.976646826\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 33.970848455243186 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 34.60550446236412\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 33.970848455243186\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1039.3525745105262\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -20135.045174503153\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 36303946.0868602\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -10590733905.84531\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 37.62093783053308 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 38.389827179654745\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 37.62093783053308\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1370.5082759219868\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -25966.98122079278\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 41719644582.6949\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -12170634163080.176\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 85.96994516067014 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 89.24627472826394\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 85.96994516067014\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -3176.1088292434165\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -72894.46028478675\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 5.9865935978919416e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.7464348355194746e+26\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.1 epochs: 1000 p_degree: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-78-3c36073289a9>:3: RuntimeWarning: overflow encountered in double_scalars\n",
            "  return lambd * np.sum(np.power((W),2)) # l2 norm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m inf \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m inf\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 197.5298300184213\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -inf\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -inf\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m inf\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -inf\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.1 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m inf \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m inf\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 326.51302084275943\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -inf\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -inf\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m inf\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -inf\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.1 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m inf \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m inf\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 509.4150341740111\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -inf\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -inf\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m inf\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -inf\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.1 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m inf \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m inf\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 733.427490857901\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -inf\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -inf\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m inf\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -inf\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.1 epochs: 1000 p_degree: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-81-bff8601229f3>:2: RuntimeWarning: invalid value encountered in subtract\n",
            "  W = W - (alpha * dW)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m nan \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 660.6287779657496\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m nan\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.1 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m nan \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1392.5917631791867\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m nan\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.1 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m nan \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2104.623721861175\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m nan\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.01 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0017022813468795736 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00015000852319247324\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0017022813468795734\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.005037235326611933\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.0013935076561983895\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003518062426253359\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.01682703330804114\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.01 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016999192413269765 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014987392404815399\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016999192413269765\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.003570375318595609\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m 0.00039224242173974133\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003512844255756519\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.015282894793751334\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.01 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.001697251419875043 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014944973586773937\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.001697251419875043\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.0021296185898189712\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m 0.001671034648693448\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.0035078281747517033\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.013899547141068558\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.01 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016956739369358188 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014906179764827407\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016956739369358188\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.0014395037118535292\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m 0.001304886179855358\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003509683780434172\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.014566578933595808\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.01 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.001694594231626668 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014878557619548043\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016945942316266677\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.0010392154687564048\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m 0.00012529072548395082\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.004981130841225884\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.44394441021117714\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.01 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.001693802716785874 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001486078497918419\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.001693802716785874\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.0007768418773534957\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.0013903661926479849\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.5771734511756914\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -167.3664531026675\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.01 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016918281113943454 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014836709534208365\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016918281112490866\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.000214457958174874\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.008202792542624904\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 8086488067449.22\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2359025082808164.5\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.001 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0017023484766233298 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00015009595111899744\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0017023484766233298\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.005030937677922509\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.0014773120668762907\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003517991927890667\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.016783831512244385\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.001 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.001700066830769242 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00015006863759155784\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.001700066830769242\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.0035347373998706466\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m 0.0004952234513738718\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.00351276355151911\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.015199023059766503\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.001 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016974772626488467 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014972952735960794\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016974772626488467\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.0020854477349940836\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m 0.0018366635145840027\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.0035077006553743914\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.013775054167117329\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.001 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016960038672490464 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001494223804482112\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016960038672490464\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.001384369427882337\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m 0.001824229179878012\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.0036667871983999036\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.06027449272619578\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.001 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016950770401521359 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014927540159529273\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016950770401521359\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.0009764845006885459\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m 0.0009535769996819043\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.07651585722893257\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -21.312191245081067\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.001 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016943188127441216 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.000149176431060759\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016943188127441216\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.0007192158343349853\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.001021783829735723\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 93.67576139267325\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -27326.486809531005\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.001 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016927203656499352 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014907967381181986\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016927203656499352\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.00020344794596582894\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.006515917688804995\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1304930669290106.8\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -3.806799879631851e+17\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.0001 epochs: 1000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 4.839132505847936 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 4.8463423050894825\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 4.839132505847936\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -157.55253930135544\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2305.0482921118783\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.869889535838592\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -84.81952110952682\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.0001 epochs: 1000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 7.465617072881484 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 7.487741742187275\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 7.465617072881484\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -169.8596023406974\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2606.3670295293423\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 7.734163381049463\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -160.1533964639056\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.0001 epochs: 1000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 13.181508987555688 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 13.200460088587956\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 13.181508987555688\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -175.49517001135027\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2659.107459072159\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 15.926817066028036\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -885.4164302010206\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.0001 epochs: 1000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 20.871476244165102 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 20.89190493330171\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 20.871476244165102\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -212.41306347872876\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -3194.341192402714\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 16944.651952786942\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4937180.474366079\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.0001 epochs: 1000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 32.86498194488006 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 32.992834815203075\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 32.86498194488006\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -207.73824181123962\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -4043.515472572675\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 7322550.907678879\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2136156561.165464\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.0001 epochs: 1000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 36.101476636838655 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 36.257005938421734\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 36.101476636838655\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -274.01198326631663\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -5217.576144705122\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 8414887017.268496\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2454827026647.723\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.0001 epochs: 1000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 1000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 82.43992618308538 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 83.09838832719402\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 82.43992618308538\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -638.6761672009335\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -14665.556131556526\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.2074982888698939e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -3.522565948447073e+25\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 5000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0018191941410942807 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0003472031318653791\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0018191941410942807\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.01598331515451168\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.38788422281765844\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.0036029337397024347\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m 0.0027084463539185366\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 5000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.002065418992453597 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0005898030734855841\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.002065418992453597\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.010931013581970461\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.4285215519570733\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.026585868201780716\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -6.632591022590976\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 5000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.002420287282496964 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0010219273338033913\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.002420287282496964\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.027809178980026106\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.8478725939120324\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 13.931295744562954\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4062.8626626414366\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 5000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0029101588467591406 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.001529954949771429\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0029101588467591406\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.02470392177492675\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1.0465401192601718\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 28.012077719145463\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -8170.427545913954\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 5000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m inf \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m inf\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 92.1562649289727\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -inf\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -inf\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m inf\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -inf\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 5000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m nan \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 209.4570770789089\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m nan\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.1 epochs: 5000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m nan \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 392.4319242620335\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m nan\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 5000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.006588717352105074 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.004781994085615802\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.006588717352105074\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.3948640496619502\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -17.508888737723534\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.03427963887525719\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -8.238852963998152\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 5000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.008530221193174366 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.005979711910968575\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.008530221193174366\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.3387077691975744\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -10.376131202108644\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.5556426614097285\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -159.73951495150152\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 5000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.010234906537622572 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.008262156811035915\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.010234906537622572\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.4310247461137977\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -16.60611047857128\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 21.62032416600155\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -6304.366458408515\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 5000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.01588699708631357 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.011354098772738047\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.01588699708631357\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -2.8540498928482707\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -14.936869381479365\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 17306.16448645407\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -5048624.9792564465\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 5000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.020710716010135074 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.016378303629300393\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.020710716010135074\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -3.321178755834037\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -23.258299757334772\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 6811171.6845803475\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1986984296.9172585\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 5000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.022911593492139948 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.01941628091001346\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.022911593492139948\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -3.682891001810427\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -35.51190696208186\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 3300826626.6987023\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -962931341936.1476\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.01 epochs: 5000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.03891862299357432 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0384232010532075\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.03891862299357432\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -2.333318819133081\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -41.96803196862378\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.478717291645436e+22\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.3065506733791499e+25\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 5000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.021910742489551386 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.02246077678538526\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.021910742489551386\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -8.942980586546312\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -144.44869056948423\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.08287083812406133\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -21.602686538988852\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 5000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.03646242052842107 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0338600921642703\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.03646242052842107\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -16.266534271204414\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -221.26647775547062\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.1673022097058803\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -337.26215908687476\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 5000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.08310423785578049 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.09118558661181064\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.08310423785578049\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -42.05813443280889\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -677.4308382491588\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 5.06777737601278\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1474.0196252908702\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 5000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.11517529135894827 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.11309717571872835\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.11517529135894827\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -57.15850429606975\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -804.6371974972009\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 96069.41400430973\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -28025776.281597387\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 5000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.2120760853764295 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.2649852071314954\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.2120760853764295\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -109.98810349512024\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2021.084178135131\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 40153647.506216735\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -11713794761.080177\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 5000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.22989952056346014 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.2363918050124044\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.22989952056346014\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -118.31371866500938\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1743.433943345026\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 44309787369.707634\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -12926241768625.527\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.001 epochs: 5000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.4738417261117129 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.6068133790637704\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.4738417261117129\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -248.4254806953557\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -4663.137899613862\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 5.503147840657661e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.6054019596925026e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 5000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.030298167447219023 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.033388740284686906\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.030298167447219023\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -13.534020670586212\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -231.0914148722788\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.09942192976350366\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -26.209403215030274\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 5000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.10343021597698285 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.10469383404046914\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.10343021597698285\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -55.36208219081396\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -807.6801336309957\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.3970366970838626\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -403.69391559132595\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 5000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.3539834566230647 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.35157623178686015\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.3539834566230647\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -201.55767382610742\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2846.0789017555626\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 17.028537629596862\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4961.5479426725715\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 5000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.6648390707373595 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.5908483102422283\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.6648390707373595\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -382.58895921189855\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -4798.963271651511\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 121898.42446361441\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -35560723.69419659\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 5000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.0928648310670859 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.4155982983545936\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.0928648310670859\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -630.3428127436507\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -11686.856714030915\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 52637186.79938742\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -15355546536.489233\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 5000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.099206109086812 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.4563272417946518\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.099206109086812\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -631.4490967335922\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -11995.53360868178\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 60370350855.00682\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -17611498432556.414\n",
            "---\n",
            "regularizer: ridge lambd: 0.001 alfa: 0.0001 epochs: 5000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.79121438958455 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 6.773587339421588\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.79121438958455\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -2217.8312790210275\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -56823.42385704123\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 8.558312695698515e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.4966677928997198e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 5000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.001642864968525087 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00015589948037220974\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016428601826492446\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.019010971285576495\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.21742528873997036\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003398320189188791\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m 0.01242712545962632\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 5000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016377666011673918 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001585450052463789\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.001637754468423925\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.02161565869145443\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.2465362271351934\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.0034064806896564547\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m 0.009821626522267257\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 5000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016353443451100202 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00016375782320047843\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016353443451100202\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.023319417499221284\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.2874742966343109\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.004995523633932592\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.45362167272570053\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 5000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016346027469055348 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00017347562268089938\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016346027469055348\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0257449058608612\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.3422608065293773\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.2488218300119115\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -71.58272134148957\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 5000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m inf \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m inf\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 93.04025405394867\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -inf\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -inf\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m inf\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -inf\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 5000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m nan \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 210.91656124664217\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m nan\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.1 epochs: 5000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m nan \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 395.0692490424976\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m nan\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 5000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.006343293906472203 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.004799239361151049\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.006343293906472203\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.2289748094372963\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -3.24168503584651\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.012240221957807126\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.315934427217465\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 5000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.009689018786137862 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.007961786881011138\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.009689018786137862\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.2258646040762584\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1.6368103238342795\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.10083915242594589\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -26.184858826932334\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 5000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.012271806763631512 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.010702029539020008\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.012271806763631512\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.2259658183066926\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2.9799055393887084\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 3.6462380152903497\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1059.7104525602163\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 5000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.018096418863240035 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.016068980959218822\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.018096418863240035\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.47009918710385534\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2.536379135981638\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 2835.6004815587576\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -827208.0030856038\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 5000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.024908072310509066 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.022951298340595054\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.024908072310509066\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.5342399465464589\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -4.046537429271106\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1117437.0578063861\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -325983537.6006549\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 5000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.02764679787655972 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.02590765409249086\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.02764679787655972\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.5821012533660275\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -6.578520012006104\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 542008103.0202863\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -158116935235.43954\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.01 epochs: 5000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.05742196135549625 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.05613768099405911\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.05742196135549625\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -0.315616060103066\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -6.6817783863974975\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 7.36389328143985e+21\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.1482266236149645e+24\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 5000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.05910070166029294 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.05933681249210174\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.05910070166029294\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -7.468195075415913\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -120.89536114825967\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.11012594832422808\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -17.989653381355836\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 5000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.08915934107137402 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.08677513545463786\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.08915934107137402\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -13.572818210969082\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -184.99039380527418\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.0337397485252915\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -281.6201858028297\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 5000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.15661347442936718 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.16316251330424136\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.15661347442936718\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -35.11413142112773\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -566.0684887082216\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.31848600320325\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1230.6230858867882\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 5000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.23589748452827433 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.2339468242978326\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.23589748452827433\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -47.72881358840446\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -672.2279565339544\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 80240.02437638944\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -23407918.861760862\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 5000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.3854579990674128 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.42946464117808353\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.3854579990674128\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -91.85652827441541\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1688.5395820532603\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 33537627.37414242\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -9783740850.183058\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 5000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.430359150122551 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.4356585533286985\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.430359150122551\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -98.81835619631961\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1457.2753815539213\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 37009539884.69746\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -10796582170447.473\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.001 epochs: 5000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.8429124700480035 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.9537314855042011\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.8429124700480035\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -207.51718750698313\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -3895.2415228068885\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.596464670768194e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.3409004453034567e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 5000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.08415610993106969 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.08716924099916294\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.08415610993106969\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -13.291087721354211\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -226.9923581199372\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.15205141977564088\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -25.73328948864371\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 5000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.1881293093412167 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.18935589339004721\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.1881293093412167\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -54.36639359607423\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -793.2686952377773\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.4586496042068298\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -396.47652555184135\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 5000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.5019209136052358 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.49954329257003693\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.5019209136052358\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -197.9516594014228\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2795.2811329376646\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 16.878967407146188\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4873.009966385814\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 5000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.896377497633132 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.8236896579030101\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.896377497633132\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -375.7502816209239\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -4713.25084291089\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 119724.07760822306\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -34926343.0167145\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 5000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.4617422274056358 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.7786998307293187\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.4617422274056358\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -619.0823720346114\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -11478.227863313783\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 51698174.43830846\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -15081613698.315207\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 5000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.507778851468158 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.8585178745228594\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.507778851468158\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -620.1701637606473\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -11781.477179777357\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 59293390797.65902\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -17297323011992.748\n",
            "---\n",
            "regularizer: ridge lambd: 0.01 alfa: 0.0001 epochs: 5000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 4.649905688367657 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 7.579003262913634\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 4.649905688367657\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -2178.261139438843\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -55809.276529973926\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 8.405638841414226e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.4521291194061667e+26\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 5000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016588776759149202 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014055778643428193\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016588661827657348\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.009114587747130254\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.09035843635168339\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003434045793823364\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m 0.0018798336284844286\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 5000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.001655560137092695 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014457576872051588\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016555455823536935\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0124410817037095\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.10577111747629453\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003428016410442574\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m 0.004283223453421914\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 5000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016527149091785523 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014626078375436704\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.001652713207892611\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.014338740792699922\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.11750807011068609\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003493990856337826\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.014873393960082915\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 5000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 46.84361848811301 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 47.031293158074334\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 46.84361848811301\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -27634.17588636438\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -392826.85874648433\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 53.17398276360845\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -15239.886426161147\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 5000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m inf \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m inf\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 101.79604643742064\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -inf\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -inf\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m inf\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -inf\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 5000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m nan \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 225.50080279971428\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m nan\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.1 epochs: 5000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m nan \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 421.25275352542025\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m nan\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 5000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016588730560998792 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001405524196364236\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016588661351637723\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.009111410560428856\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.0903970504994751\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.0034340185337811313\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m 0.001884898420042691\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 5000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.001655553243014155 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001445536783373336\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016555455454196437\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.012436232051074692\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.10571028672797533\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.0034279787620722976\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m 0.004289844707132895\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 5000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016527132113677801 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014625386796364426\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016527131959302988\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.014336790597533344\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.11749121673886798\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.0034978092581555847\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.015988755072698924\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 5000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016519685692059723 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001477278107126468\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016519673439959128\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.014842081883027602\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.1292422888065543\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.01834448807965425\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4.34709808997812\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 5000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016518302998451686 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001502192397031516\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016518302998451686\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.01544682028222466\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.14308857118421336\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.465592743460585\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1301.717236080043\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 5000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016517693825429927 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00015310843386497227\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016517693825429927\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.016276629698580014\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.1564787652859594\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1583.2280232998785\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -461865.08409008774\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.01 epochs: 5000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016507189118598197 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001627460789268783\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016507189118598197\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.02022817647328745\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.19160989732760725\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.9873827971656172e+16\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -5.797678582532064e+18\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 5000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.07812982422279904 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.07697241942185612\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.07812982422279904\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.2266143649450045\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -20.659798658715765\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.08752727085362108\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.82065483729348\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 5000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.11271237835455136 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.1111592724067316\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.11271237835455136\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -2.196183803387404\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -31.014158928489074\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.2695724725218269\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -46.309021783419766\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 5000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.17094899805149397 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.17090087933217218\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.17094899805149397\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -5.752097982442642\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -94.17882555292516\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.8555929608685229\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -201.99987602096593\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 5000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.2708038321290588 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.26931081452937244\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.2708038321290588\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -7.841700765957608\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -111.44924298373006\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 13252.244802529694\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -3865926.1432247716\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 5000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.4089196668527817 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.41510176647922076\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.4089196668527817\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -15.136995566786673\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -280.12730906508546\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 5539295.21307737\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1615946914.1910007\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 5000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.46592625008035127 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.46588547539450154\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.46592625008035127\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -16.309252160423405\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -243.69808357310066\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 6114116838.390873\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1783636463646.464\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.001 epochs: 5000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.8790966105198008 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.8961663132073069\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.8790966105198008\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -34.34120035057211\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -644.7721483621905\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 7.593288671240035e+22\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.215146833464466e+25\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 5000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.5246932452976047 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.5270015763526332\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.5246932452976047\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -11.088428310849917\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -189.8039467883684\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.5814591065148679\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -21.418978021270746\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 5000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.8800464269999095 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.8809309566353323\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.8800464269999095\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -45.33947197922898\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -662.560753609205\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.941201501231745\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -331.02445133517534\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 5000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.7076903700774273 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.7055746256585\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.7076903700774273\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -165.25112003427634\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -2334.573980875344\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 15.386496241017301\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4070.018369342212\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 5000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.781779269273941 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.7209032794261216\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.781779269273941\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -313.73218149239653\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -3935.9142884067696\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 100003.39753761007\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -29172761.322304085\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 5000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 4.464936249149517 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 4.7295038668157385\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 4.464936249149517\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -516.9615180223309\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -9586.100065712739\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 43181702.99035223\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -12597151088.585245\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 5000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 4.835895892285434 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 5.128743521757018\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 4.835895892285434\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -517.8810372658072\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -9840.089981258172\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 49525784134.76825\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -14447874780223.59\n",
            "---\n",
            "regularizer: ridge lambd: 0.1 alfa: 0.0001 epochs: 5000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 11.621210402446838 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 14.067139903910327\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 11.621210402446838\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1819.3785831785553\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -46611.56099582617\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 7.020946870345032e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2.048180821397277e+26\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 5000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016742826214747107 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013724227228306557\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016742826066954747\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0018918768521011264\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.03309373593361986\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003466774723561778\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.006674683554097394\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 5000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016721877161872227 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013812766165668836\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016715528205445084\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0034054751818412\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.03706078133247925\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003462390664259378\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.0052732736923579715\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 5000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.605704017777622e+199 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.6114240857177394e+199\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 107.66447136663338\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -7.943763108116442e+201\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1.1294654188198208e+203\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.6557024338090064e+199\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -3.99602576654027e+201\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 5000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m nan \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 175.29512067182796\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m nan\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 5000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m nan \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 183.22279227141755\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m nan\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 5000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m nan \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 373.2117416906867\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m nan\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.1 epochs: 5000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m nan \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 669.7828300922861\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m nan\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m nan\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m nan\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 5000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016742826214747107 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013724227228306555\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016742826175260878\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0018918768521011264\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.03309373593361986\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003466774723561778\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.006674683554097394\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 5000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016721877161872227 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001381276616566883\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016721877161827788\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.003405475181840978\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.03706078133247881\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003462390664259378\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.0052732736923579715\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 5000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016700450294052228 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001385142612318079\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016700450294052194\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0047260716656429835\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.039917120479308954\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003460154736008198\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.00460600865759786\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 5000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016689496386177798 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013842517119635892\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016689496386141456\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0051573569957934495\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.04238608414437972\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003667604275981077\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.0652345983417284\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 5000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016682944911124516 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013841290466377434\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.001668294490950493\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.005325744637290963\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.04548018751536653\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.07512497674598026\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -20.911196175552007\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 5000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016678712289123078 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013856514139727347\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.001667871211148771\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.005432534458082117\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.04887216717623866\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 27.178868818149734\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -7927.732007201301\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.01 epochs: 5000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016670744452356979 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00014022962804372268\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016670731316433837\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.005950809621104947\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.0625073361896491\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 364610804106969.4\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -1.0636583213589174e+17\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 5000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.00167429610218482 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001372701109140458\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.00167429610218482\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0018932675162155466\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.03319639145037723\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.0034666787453791563\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.006642077710595329\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 5000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016722238649613597 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013818356771786347\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016722238649613597\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0034175291352153137\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.037058485315076384\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.0034622846316834733\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.005225953649772519\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 5000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016700891098343146 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013858498286221748\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016700891098343146\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.004738095278009946\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.039973911436998355\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003459868639745223\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.004503860510803426\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 5000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016690068575900745 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013849854945348916\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016690068575900745\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0051685651200849\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.0423651040818942\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.0034584574343386725\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.004199257561872782\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 5000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016683705909079386 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00013851802439433873\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016683705909079386\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.0053367831951305345\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.045571192855632825\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003857871382293947\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -0.12082209535740862\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 5000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.0016679477915303629 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001387050645713103\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016679477915303629\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.005438863440679342\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.04932246001692997\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 18.864156686553315\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -5502.128393298853\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.001 epochs: 5000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.001667172532632136 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.0001403324441849703\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.001667172532632136\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.005935116669319007\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.0627697932575979\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 212507707791716.53\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -6.199366261216283e+16\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 5000 p_degree: 1\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.8386527455995838 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.8378722596523732\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.8386527455995838\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -1.7947697404055964\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -31.914827101151445\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.8488506568511387\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -3.3295423037121843\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 5000 p_degree: 2\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 1.3412367134420515 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 1.3404077627748787\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 1.3412367134420515\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -7.291543025708233\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -109.3191858372114\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.5170460816535798\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -54.30659468930874\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 5000 p_degree: 4\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 2.412685615463932 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 2.411391924527825\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 2.412685615463932\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -27.05013587177707\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -385.0797781740753\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 4.672864814150223\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -671.9444852888761\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 5000 p_degree: 6\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 3.820813032789199 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 3.809712270408882\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 3.820813032789199\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -51.50819460413013\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -647.7648153177224\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 16529.67063324108\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -4821016.444543377\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 5000 p_degree: 8\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 6.100226976487127 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 6.142847945346823\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 6.100226976487127\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -85.04362074200773\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1580.2826217397733\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 7136130.208578538\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2081780861.403239\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 5000 p_degree: 10\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 6.711033149023747 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 6.758513685404601\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 6.711033149023747\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -85.22650099740822\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -1624.2797381727578\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 8184679419.949314\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -2387669884609.4995\n",
            "---\n",
            "regularizer: ridge lambd: 1 alfa: 0.0001 epochs: 5000 p_degree: 20\n",
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 14.708377434547899 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 15.1101540877333\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 14.708377434547899\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m -300.5423994235423\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -7691.424465463138\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 1.160285580534403e+23\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m -3.3848350048511076e+25\n",
            "---\n",
            "regularizer: ridge lambd: 5 alfa: 0.1 epochs: 5000 p_degree: 1\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-91-f76dc6cf041c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mregularizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m \u001b[1;31m#\"lasso\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_pow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr2_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr2_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_output_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malfa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_degree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mloss_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr2_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_output_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_pow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"regularizer\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"lambd\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"alfa\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"epochs\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"p_degree\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"ratio\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"loss_train\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"loss_val\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"loss_test\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mloss_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r2_train\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mr2_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r2_val\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mr2_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r2_test\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mr2_test\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-89-f4c58608070c>\u001b[0m in \u001b[0;36mtrain_output_print\u001b[1;34m(X_train, Y_train, X_validation, Y_validation, X_test, Y_test, epochs, alfa, p_degree, lambd, regularizer, ratio)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_output_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malfa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_degree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"ridge\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_pow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val_pow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_pow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epochs_3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malfa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_degree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{Fore.GREEN}{Style.BRIGHT}• with epochs:{Style.RESET_ALL}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"->\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"{Fore.RED}{Style.BRIGHT}\\tlast training loss:{Style.RESET_ALL}\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34mf\"{Fore.RED}{Style.BRIGHT}\\n\\t\\t\\tlast validation loss:{Style.RESET_ALL}\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-84-2e39a4345de3>\u001b[0m in \u001b[0;36mtrain_epochs_3d\u001b[1;34m(X, Y, X_val, Y_val, X_train, Y_train, epochs, alfa, p_degree, lambd, regularizer, ratio)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malfa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mL_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mL_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mloss_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mloss_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-79-4bddb9028ed6>\u001b[0m in \u001b[0;36mError\u001b[1;34m(X, W, Y, lambd, regularizer, ratio)\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[1;31m# print(features_len)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m   \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m   \u001b[0ml2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# l2 norm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-77-eb15c5bfe249>\u001b[0m in \u001b[0;36mh\u001b[1;34m(X, W)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# multi X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ijk,ik->j\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36meinsum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(columns=[\"regularizer\",\"lambd\",\"alfa\",\"epochs\",\"p_degree\",\"ratio\", \"loss_train\", \"loss_val\", \"loss_test\", \"r2_train\", \"r2_val\", \"r2_test\"])\n",
        "for r in [\"ridge\"]:\n",
        "  for e in [100, 1000, 5000]:\n",
        "    for l in [0.001, 0.01, 0.1, 1, 5]:\n",
        "      for a in [0.1, 0.01, 0.001, 0.0001]:\n",
        "        for p in [1,2,4,6,8,10,20]:\n",
        "          if r == \"elasticnet\":\n",
        "\n",
        "            for ra in [0.3, 0.5, 0.7, 0.9]:\n",
        "              print(\"---\\nregularizer:\",r,\"lambd:\",l,\"alfa:\",a,\"epochs:\",e,\"p_degree:\", p, \"ratio:\", ra)\n",
        "\n",
        "              epochs = e \n",
        "              alfa = a \n",
        "              p_degree = p \n",
        "              lambd = l \n",
        "              regularizer = r \n",
        "              ratio = ra\n",
        "              W, loss_train, loss_val, X_test_pow, r2_train, r2_val = train_output_print(X_train, Y_train, X_validation, Y_validation, X_test, Y_test, epochs, alfa, p_degree, lambd, regularizer, ratio)\n",
        "              loss_test, r2_test = test_output_print(X_test_pow, W, Y_test, lambd, regularizer, ratio)\n",
        "\n",
        "              df = df.append({\"regularizer\":r,\"lambd\":l,\"alfa\":a,\"epochs\":e,\"p_degree\":p,\"ratio\":ra, \"loss_train\":loss_train[-1], \"loss_val\":loss_val[-1], \"loss_test\":loss_test, \"r2_train\":r2_train, \"r2_val\":r2_val, \"r2_test\":r2_test}, ignore_index=True)\n",
        "          else:\n",
        "            print(\"---\\nregularizer:\",r,\"lambd:\",l,\"alfa:\",a,\"epochs:\",e,\"p_degree:\",p)\n",
        "            epochs = e \n",
        "            alfa = a \n",
        "            p_degree = p \n",
        "            lambd = l \n",
        "            regularizer = r \n",
        "            ratio = 0.3 \n",
        "            W, loss_train, loss_val, X_test_pow, r2_train, r2_val = train_output_print(X_train, Y_train, X_validation, Y_validation, X_test, Y_test, epochs, alfa, p_degree, lambd, regularizer, ratio)\n",
        "            loss_test, r2_test = test_output_print(X_test_pow, W, Y_test, lambd, regularizer, ratio)\n",
        "            df = df.append({\"regularizer\":r,\"lambd\":l,\"alfa\":a,\"epochs\":e,\"p_degree\":p,\"ratio\":0, \"loss_train\":loss_train[-1], \"loss_val\":loss_val[-1], \"loss_test\":loss_test, \"r2_train\":r2_train, \"r2_val\":r2_val, \"r2_test\":r2_test}, ignore_index=True)\n",
        "\n",
        "df.to_csv(\"ridge_tests.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ridge testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Completed!\n",
            "-------------------------------\n",
            "\n",
            "\u001b[32m\u001b[1m• with epochs:\u001b[0m 5000 -> \u001b[31m\u001b[1m\tlast training loss:\u001b[0m 0.001642864968525087 \u001b[31m\u001b[1m\n",
            "\t\t\tlast validation loss:\u001b[0m 0.00015589948037220974\n",
            "\u001b[32m\u001b[1m\t\t\tmin training loss:\u001b[0m 0.0016428601826492446\n",
            "\u001b[31m\u001b[1m• R^2 training:\u001b[0m 0.019010971285576495\n",
            "\u001b[31m\u001b[1m• R^2 validation:\u001b[0m -0.21742528873997036\n",
            "\u001b[32m\u001b[1m• test loss on current model:\n",
            "\u001b[0m 0.003398320189188791\n",
            "\u001b[31m\u001b[1m• R^2 test:\u001b[0m 0.01242712545962632\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApQAAAJ9CAYAAACLhOJ2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABW4klEQVR4nO3deXhU5d3/8c83CYQdwqaSIItjAwTZHIK2rlgWsY1VEYKIKFiqYltp7QNdHtT+tPC0Fa1KtVRQ6kJUag21EFTqXjUGRStBDJpQElxChLAHkty/PzKkARK2M0vgvF/XNVdmzrnPfX/PnDp8elZzzgkAAAA4VnGxLgAAAADHNwIlAAAAPCFQAgAAwBMCJQAAADwhUAIAAMATAiUAAAA8IVACQCNmZqea2XYzi491LQDQEAIlgCNmZkVm9u1Y1xFNZubMLBB6f7uZPR7h8fb7jp1z/3HOtXLOVUVyXADwgkAJAJLMLOFEGAMAYoFACcAzM0s0s3vNbGPoda+ZJYbmdTSz581si5l9bWavm1lcaN50Mysxs21mttbMLmqg/7Zm9hczKzWz9Wb2KzOLC427xcz61mnbycx2mVnn0OfvmNmqULt/mVm/Om2LQjV8KGnHoQKfmY2U9AtJY0OHoD+oU9t8M/s8tC537js8bWbXmtmbZnaPmZVJut3MTjOzf5pZmZltMrMnzKxdqP1jkk6V9PfQGP9jZt1De0kTQm26mNmS0He5zsy+X6fG283s6dB3tc3MVptZ8Bg2KQAcFQIlgHD4paSzJA2Q1F9SuqRfheb9VFKxpE6STlJNKHNmlirpZkmDnXOtJY2QVNRA//dLaiupp6TzJV0j6TrnXIWkZyWNq9N2jKRXnXNfmdlASQsk/UBSB0l/krRkX9gNGSfpEkntnHOVDa2gcy5H0m8kPRU6BN0/NOtRSZWSApIGShou6fo6iw6R9Flo3e+SZJJmSeoiqbekrpJuD40xQdJ/JH03NMZv6yklSzXfZxdJoyX9xsyG1pmfEWrTTtISSQ80tE4AEC4ESgDhMF7Sr51zXznnSiXdIWlCaN5eSadI6uac2+uce9055yRVSUqU1MfMmjjnipxznx7YcWhvX6aknzvntjnniiTdXaf/J0Pz97kqNE2Spkj6k3PuHedclXNuoaQK1YTffe5zzm1wzu062pU2s5MkjZJ0i3Nuh3PuK0n3HFDPRufc/c65SufcLufcOufci865itB3NUc1IflIxusq6VuSpjvndjvnVkl6WDUBe583nHNLQ+dcPqaagA8AEUWgBBAOXSStr/N5fWiaJP1O0jpJL5jZZ2Y2Q5Kcc+sk3aKavXNfmVmWmXXRwTpKalJP/8mh9y9LamFmQ8ysu2r2kv4tNK+bpJ+GDndvMbMtqtkjWHecDUe7snV0C9X2eZ3+/ySpc0P9m9lJoXUtMbOtkh4PreOR6CLpa+fctjrT6n4XkvRFnfc7JTXj3E0AkUagBBAOG1UTrvY5NTRNob2KP3XO9VTN4dif7DtX0jn3pHPunNCyTtL/1dP3JtXs5Tyw/5JQH1WSnlbNoetxkp6vE7g2SLrLOdeuzquFc25Rnb7cUazngW03qGaPZ8c6/bdxzqUdYpnfhKad4ZxrI+lq1RwGP5J6Nkpqb2at60yr/S4AIFYIlACOVhMza1bnlSBpkaRfhS6I6Shppmr2vO27KCZgZiapXDWHuqvNLNXMhobOZ9wtaZek6gMHqxMY7zKz1mbWTdJP9vUf8qSksao59P5knel/lnRDaO+lmVlLM7vkgEB2NL6U1H3fRUXOuc8lvSDpbjNrE7pQ6DQzO9Qh7NaStksqN7NkST+rZ4ye9S3onNsg6V+SZoW++36SJmv/7wIAoo5ACeBoLVVN+Nv3ul3SnZLyJH0o6d+S3gtNk6TTJb2kmhD1lqQ/OudeVs35k7NVswfyC9UcJv55A2P+UNIO1Vzc8oZqQuOCfTOdc++E5neRtKzO9DxJ31fNhSmbVXPo/dpjXnPpmdDfMjN7L/T+GklNJeWHxlismnNGG3KHpEGqCdf/UM1FRXXNUk0432Jmt9az/DhJ3VWzt/Jvkm5zzr109KsCAOFjNefGAwAAAMeGPZQAAADwhEAJAAAATwiUAAAA8IRACQAAAE8IlAAAAPCkUT09oWPHjq579+6xLgMAACBqVq5cuck51ynWdXjRqAJl9+7dlZeXF+syAAAAosbM1h++VePGIW8AAAB4QqAEAACAJwRKAAAAeNKozqEEAACNw969e1VcXKzdu3fHupQTRrNmzZSSkqImTZrEupSwI1ACAICDFBcXq3Xr1urevbvMLNblHPeccyorK1NxcbF69OgR63LCjkPeAADgILt371aHDh0Ik2FiZurQocMJu8eXQAkAAOpFmAyvE/n7JFACAIBGZ8uWLfrjH/941MuNGjVKW7ZsCX9BOCQCJQAAaHQaCpSVlZWHXG7p0qVq165dhKpCQ7goBwAANDozZszQp59+qgEDBqhJkyZq1qyZkpKS9PHHH+uTTz7R9773PW3YsEG7d+/Wj3/8Y02ZMkXSf5+6t337dl188cU655xz9K9//UvJycnKzs5W8+bNY7xmJyYCJQAAOKQ7/r5a+Ru3hrXPPl3a6LbvpjU4f/bs2froo4+0atUqvfLKK7rkkkv00Ucf1V4hvWDBArVv3167du3S4MGDdcUVV6hDhw779VFQUKBFixbpz3/+s8aMGaO//vWvuvrqq8O6HqhBoAQAAI1eenr6frfbue+++/S3v/1NkrRhwwYVFBQcFCh79OihAQMGSJLOPPNMFRUVRatc3yFQAgCAQzrUnsRoadmyZe37V155RS+99JLeeusttWjRQhdccEG9t+NJTEysfR8fH69du3ZFpVY/4qIcAADQ6LRu3Vrbtm2rd155ebmSkpLUokULffzxx3r77bejXB0OxB5KAADQ6HTo0EHf+ta31LdvXzVv3lwnnXRS7byRI0fqoYceUu/evZWamqqzzjorhpVCksw5F+saagWDQZeXlxfrMgAA8L01a9aod+/esS7jhFPf92pmK51zwRiVFBYc8gYAAIAnBEoAAAB4QqAEAACAJwRKAAAAeEKgBAAAgCcESgAAAHhCoAQAAMe9Vq1aSZI2btyo0aNH19vmggsu0OFuT3jvvfdq586dtZ9HjRqlLVu2hK3OE5XvAuXoB/+lRbn/iXUZAAAgArp06aLFixcf8/IHBsqlS5eqXbt2YajsxOa7QLnyP5v1+Rae5QkAQGM2Y8YMzZ07t/bz7bffrjvvvFMXXXSRBg0apDPOOEPZ2dkHLVdUVKS+fftKknbt2qXMzEz17t1bl1122X7P8r7xxhsVDAaVlpam2267TZJ03333aePGjbrwwgt14YUXSpK6d++uTZs2SZLmzJmjvn37qm/fvrr33ntrx+vdu7e+//3vKy0tTcOHD/flM8N59CIAADi0ZTOkL/4d3j5PPkO6eHaDs8eOHatbbrlFU6dOlSQ9/fTTWr58uX70ox+pTZs22rRpk8466yxlZGTIzOrt48EHH1SLFi20Zs0affjhhxo0aFDtvLvuukvt27dXVVWVLrroIn344Yf60Y9+pDlz5ujll19Wx44d9+tr5cqVeuSRR/TOO+/IOachQ4bo/PPPV1JSkgoKCrRo0SL9+c9/1pgxY/TXv/5VV199dRi+pOOH7/ZQ/iD+7+qydVWsywAAAIcwcOBAffXVV9q4caM++OADJSUl6eSTT9YvfvEL9evXT9/+9rdVUlKiL7/8ssE+Xnvttdpg169fP/Xr16923tNPP61BgwZp4MCBWr16tfLz8w9ZzxtvvKHLLrtMLVu2VKtWrXT55Zfr9ddflyT16NFDAwYMkCSdeeaZKioq8rbyxyHf7aH8n/gs5W5pIWlsrEsBAOD4cIg9iZF05ZVXavHixfriiy80duxYPfHEEyotLdXKlSvVpEkTde/eXbt37z7qfgsLC/X73/9e7777rpKSknTttdceUz/7JCYm1r6Pj4/35SFv3+2hBAAAx4exY8cqKytLixcv1pVXXqny8nJ17txZTZo00csvv6z169cfcvnzzjtPTz75pCTpo48+0ocffihJ2rp1q1q2bKm2bdvqyy+/1LJly2qXad26tbZt23ZQX+eee66ee+457dy5Uzt27NDf/vY3nXvuuWFc2+Ob7/ZQAgCA40NaWpq2bdum5ORknXLKKRo/fry++93v6owzzlAwGFSvXr0OufyNN96o6667Tr1791bv3r115plnSpL69++vgQMHqlevXuratau+9a1v1S4zZcoUjRw5Ul26dNHLL79cO33QoEG69tprlZ6eLkm6/vrrNXDgQF8e3q6POediXUOtYDDoDnd/KK+qb2un3K6TdNb1cyI6DgAAx7M1a9aod+/esS7jhFPf92pmK51zwRiVFBYc8gYAAIAnvgyUjWefLAAAwPHPl4ESAAAA4UOgBAAA9WpM11mcCE7k79OngfLE3aAAAIRDs2bNVFZWdkKHoGhyzqmsrEzNmjWLdSkR4bvbBvGfBQAAh5eSkqLi4mKVlpbGupQTRrNmzZSSkhLrMiLCd4ESAAAcXpMmTdSjR49Yl4HjhE8PeQMAACBcCJQAAADwhEAJAAAAT/wZKLkyBwAAIGz8GSgBAAAQNgRKAAAAeOLTQMkxbwAAgHDxXaB0sliXAAAAcELxXaAEAABAeBEoAQAA4AmBEgAAAJ4QKAEAAOAJgRIAAACeECgBAADgCYESAAAAnhAoAQAA4IlPAyVPygEAv7rgggv08MMP1zvvN7/5ja6//vpj6vfaa6/Vr371Ky+lRVRRUZHMTJWVlbEuBScg3wVKnpQDAGjIL37xiwbDZjQ8+uijOuecc2I2PnCsfBcoAQAAEF4ESgDAMXvqqafUqlWr2ldiYqIuuOACSVJ5ebmuueYaderUSd26ddOdd96p6upqSVJ1dbXuvPNOdevWTZ07d9Y111yj8vJySf89NPvII4+oa9euSkpK0kMPPaR3331X/fr1U7t27XTzzTfvV8eCBQvUu3dvJSUlacSIEVq/fn3tvBdffFG9evVS27ZtdfPNN8u5hk97uv3223X11VfvV8fChQt16qmnqmPHjrrrrruO6HvZvHmzvvOd76hTp05KSkrSd77zHRUXF9fOf/TRR9WzZ0+1bt1aPXr00BNPPKE1a9bohhtu0FtvvaVWrVqpXbt29X7fwWBwv2n33HOPMjIyJEn/+Mc/NHDgQLVp00Zdu3bV7bff3mCN3bt310svvVTvukvS22+/rW9+85tq166d+vfvr1deeeWQ9cPfCJQAgGM2duxYbd++Xdu3b9fGjRvVs2dPjRs3TpL0wx/+UOXl5frss8/06quv6i9/+YseeeQRSTWB5NFHH9XLL7+szz77TNu3bz8oJL7zzjsqKCjQU089pVtuuUV33XWXXnrpJa1evVpPP/20Xn31VUlSdna2fvOb3+jZZ59VaWmpzj333NoaNm3apMsvv1x33nmnNm3apNNOO01vvvnmUa3jG2+8obVr12rFihX69a9/rTVr1hx2merqal133XVav369/vOf/6h58+a167djxw796Ec/0rJly7Rt2zb961//0oABA9S7d2899NBDOvvss7V9+3Zt2bLloH6/+93vau3atSooKKid9uSTT+qqq66SJLVs2VJ/+ctftGXLFv3jH//Qgw8+qOeee+6o1leSSkpKdMkll+hXv/qVvv76a/3+97/XFVdcodLS0gbrh78RKAEAnlVXV+uqq67SBRdcoB/84AeqqqpSVlaWZs2apdatW6t79+766U9/qscee0yS9MQTT+gnP/mJevbsqVatWmnWrFnKysra74KR//3f/1WzZs00fPhwtWzZUuPGjVPnzp2VnJysc889V++//74k6aGHHtLPf/5z9e7dWwkJCfrFL36hVatWaf369Vq6dKnS0tI0evRoNWnSRLfccotOPvnko1q32267Tc2bN1f//v3Vv39/ffDBB4ddpkOHDrriiivUokULtW7dWr/85S9rA7AkxcXF6aOPPtKuXbt0yimnKC0t7YhqadGihS699FItWrRIklRQUKCPP/64dg/lBRdcoDPOOENxcXHq16+fxo0bt9+4R+rxxx/XqFGjNGrUKMXFxWnYsGEKBoNaunSpp/px4vJnoOQibwAIq1/+8pfatm2b7rvvPkk1ewb37t2rbt261bbp1q2bSkpKJEkbN248aF5lZaW+/PLL2mknnXRS7fvmzZsf9Hn79u2SpPXr1+vHP/6x2rVrp3bt2ql9+/ZyzqmkpEQbN25U165da5czs/0+H4m6AbRFixa14x7Kzp079YMf/EDdunVTmzZtdN5552nLli2qqqpSy5Yt9dRTT+mhhx7SKaecoksuuUQff/zxEddz1VVX1QbKJ598Ut/73vfUokULSTV7dS+88EJ16tRJbdu21UMPPaRNmzYd1fpKNd/pM888U/udtmvXTm+88YY+//xzz/XjxOTPQAkACJusrCwtWrRIixcvVpMmTSRJHTt2VJMmTfY7l/E///mPkpOTJUldunQ5aF5CQsJ+ofFIde3aVX/605+0ZcuW2teuXbv0zW9+U6eccoo2bNhQ29Y5t9/nSLn77ru1du1avfPOO9q6datee+212vElacSIEXrxxRf1+eefq1evXvr+978vqSbwHs6wYcNUWlqqVatWadGiRbWHu6WasJmRkaENGzaovLxcN9xwQ4PnjLZs2VI7d+6s/fzFF1/Uvu/atasmTJiw33e6Y8cOzZgx45D1w78IlACAY/b+++/rhz/8oZ577jl16tSpdnp8fLzGjBlTu+dy/fr1mjNnTu1FH+PGjdM999yjwsJCbd++Xb/4xS80duxYJSQkHHUNN9xwg2bNmqXVq1dLqrkY6JlnnpEkXXLJJVq9erWeffZZVVZW6r777tsvOEXKtm3b1Lx5c7Vr105ff/217rjjjtp5X375pbKzs7Vjxw4lJiaqVatWiour+ef4pJNOUnFxsfbs2dNg302aNNGVV16pn/3sZ/r66681bNiw/cZt3769mjVrptzcXD355JMN9jNgwABlZWVp7969ysvL0+LFi2vnXX311fr73/+u5cuXq6qqSrt379Yrr7yi4uLiQ9YP//Lp/wI45g0A4ZCdna3NmzfrnHPOqb3S++KLL5Yk3X///WrZsqV69uypc845R1dddZUmTZokSZo0aZImTJig8847Tz169FCzZs10//33H1MNl112maZPn67MzEy1adNGffv21bJlyyTV7Cl95plnNGPGDHXo0EEFBQX61re+FZ6VP4RbbrlFu3btUseOHXXWWWdp5MiRtfOqq6s1Z84cdenSRe3bt9err76qBx98UJI0dOhQpaWl6eSTT1bHjh0b7P+qq67SSy+9pCuvvHK/EP7HP/5RM2fOVOvWrfXrX/9aY8aMabCP//f//p8+/fRTJSUl6bbbbttvT2fXrl1rL3bq1KmTunbtqt/97neqrq4+ZP3wLzvU7ROiLRgMury8vIiOsee29lqZfLXOnnJfRMcBAAA4Ema20jkXPHzLxsuneygBAAAQLgRKAAAAeEKgBAAAgCcESgAAAHhCoAQAAIAnR3/DLwA4AXXs2FHdu3ePdRkA/Kl/rAvwikAJAJK6d++uSN+2DADqY2YN38n+OOHTQ96N596bAAAAh2NmI81srZmtM7MZ9cxPNLOnQvPfMbPudeb9PDR9rZmNqDN9gZl9ZWYf1dPfD83sYzNbbWa/PVx9PgyUh39OKgAAQGNhZvGS5kq6WFIfSePMrM8BzSZL2uycC0i6R9L/hZbtIylTUpqkkZL+GOpPkh4NTTtwvAslXSqpv3MuTdLvD1ejDwMlAADAcSVd0jrn3GfOuT2SslQT+Oq6VNLC0PvFki4yMwtNz3LOVTjnCiWtC/Un59xrkr6uZ7wbJc12zlWE2n11uAIJlAAAAI1bsqQNdT4Xh6bV28Y5VympXFKHI1z2QN+QdG7o0PmrZjb4cAVyUQ4AAADqSpDUXtJZkgZLetrMejrnGrwIhT2UAAAAjVuJpK51PqeEptXbxswSJLWVVHaEyx6oWNKzrkaupGpJHQ+1gD8DJRd5AwCA48e7kk43sx5m1lQ1F9ksOaDNEkkTQ+9HS/pnaI/iEkmZoavAe0g6XVLuYcZ7TtKFkmRm35DUVNKmQy3AIW8AOAZ3/H218jdujXUZABqRPl3a6LbvpoW9X+dcpZndLGm5pHhJC5xzq83s15LynHNLJM2X9JiZrVPNhTaZoWVXm9nTkvIlVUqa6pyrkiQzWyTpAkkdzaxY0m3OufmSFkhaELqd0B5JEw91uFsiUAIAADR6zrmlkpYeMG1mnfe7JV3ZwLJ3SbqrnunjGmi/R9LVR1MfgRIAjkEk9kIAwPHKd+dQOm5sDkRdTk6OUlNTFQgENHv27IPmV1RUaOzYsQoEAhoyZIiKiopq582aNUuBQECpqalavnx57fRJkyapc+fO6tu370H93X///erVq5fS0tL0P//zPxFZJwDAf/kuUAKIrqqqKk2dOlXLli1Tfn6+Fi1apPz8/P3azJ8/X0lJSVq3bp2mTZum6dOnS5Ly8/OVlZWl1atXKycnRzfddJOqqqokSddee61ycnIOGu/ll19Wdna2PvjgA61evVq33npr5FcSAHyOQAkgonJzcxUIBNSzZ081bdpUmZmZys7O3q9Ndna2Jk6suThx9OjRWrFihZxzys7OVmZmphITE9WjRw8FAgHl5tZcnHjeeeepffv2B4334IMPasaMGUpMTJQkde7cOcJrCAAgUAKIqJKSEnXt+t9boKWkpKikpKTBNgkJCWrbtq3KysqOaNkDffLJJ3r99dc1ZMgQnX/++Xr33XcbbDtv3jwFg0EFg0GVlpYey+oBAMRFOQBOMJWVlfr666/19ttv691339WYMWP02WefqeaRtvubMmWKpkyZIkkKBoPRLhUAThg+3UPJnc2BaElOTtaGDf99jGxxcbGSk5MbbFNZWany8nJ16NDhiJY9UEpKii6//HKZmdLT0xUXF6dNmw55P14AgEc+DZQAomXw4MEqKChQYWGh9uzZo6ysLGVkZOzXJiMjQwsXLpQkLV68WEOHDpWZKSMjQ1lZWaqoqFBhYaEKCgqUnp5+yPG+973v6eWXX5ZUc/h7z5496tjxkE8MAwB4FNFAaWbTzGy1mX1kZovMrFkkxwPQ+CQkJOiBBx7QiBEj1Lt3b40ZM0ZpaWmaOXOmliypeXLY5MmTVVZWpkAgoDlz5tTeWigtLU1jxoxRnz59NHLkSM2dO1fx8fGSpHHjxunss8/W2rVrlZKSovnz50uquZ3QZ599pr59+yozM1MLFy6s93A3ACB87DBP0jn2js2SJb0hqY9zblfosT9LnXOPNrRMMBh0eXl5Ealnn4rbOuq9LmN19g/mRnQcAMeXYDCoSP/+AEB9zGync65lrOvwItKHvBMkNTezBEktJG2M8HiHxdmTAAAA4RWxQOmcK5H0e0n/kfS5pHLn3AuRGg8AAACxEbFAaWZJki6V1ENSF0ktzeygB42b2RQzyzOzPO4DBwAAcPyJ5CHvb0sqdM6VOuf2SnpW0jcPbOScm+ecCzrngp06dYpgOQAAAIiESAbK/0g6y8xaWM0llhdJWhPB8QAAABADkTyH8h1JiyW9J+nfobHmRWo8AAAAxEZEH73onLtN0m2RHAMAAACxxZNyAAAA4AmBEgAAAJ74MlBahJ4OBAAA4Ee+C5ROPNMXAAAgnHwXKAEAABBeBEoAAAB4QqAEAACAJwRKAAAAeOLLQMk13gAAAOHjy0AJAACA8CFQAoiKnJwcpaamKhAIaPbs2QfNr6io0NixYxUIBDRkyBAVFRXVzps1a5YCgYBSU1O1fPny2umTJk1S586d1bdv33rHvPvuu2Vm2rRpU9jXBwDwX74MlNyJEoiuqqoqTZ06VcuWLVN+fr4WLVqk/Pz8/drMnz9fSUlJWrdunaZNm6bp06dLkvLz85WVlaXVq1crJydHN910k6qqqiRJ1157rXJycuodc8OGDXrhhRd06qmnRnblAAD+C5Tc2ByIvtzcXAUCAfXs2VNNmzZVZmamsrOz92uTnZ2tiRMnSpJGjx6tFStWyDmn7OxsZWZmKjExUT169FAgEFBubq4k6bzzzlP79u3rHXPatGn67W9/KzP+mweASPNdoAQQfSUlJeratWvt55SUFJWUlDTYJiEhQW3btlVZWdkRLXug7OxsJScnq3///mFcCwBAQxJiXQAAhNPOnTv1m9/8Ri+88MJh286bN0/z5s2TJJWWlka6NAA4YbGHEkDEJScna8OGDbWfi4uLlZyc3GCbyspKlZeXq0OHDke0bF2ffvqpCgsL1b9/f3Xv3l3FxcUaNGiQvvjii4PaTpkyRXl5ecrLy1OnTp28riYA+BaBEkDEDR48WAUFBSosLNSePXuUlZWljIyM/dpkZGRo4cKFkqTFixdr6NChMjNlZGQoKytLFRUVKiwsVEFBgdLT0xsc64wzztBXX32loqIiFRUVKSUlRe+9955OPvnkiK4jAPiZLwOl49bmQFQlJCTogQce0IgRI9S7d2+NGTNGaWlpmjlzppYsWSJJmjx5ssrKyhQIBDRnzpzaWwulpaVpzJgx6tOnj0aOHKm5c+cqPj5ekjRu3DidffbZWrt2rVJSUjR//vyYrSMA+Jk513jCVTAYdHl5eREdY+dtnfXBKVfo7BsejOg4AI4vwWBQkf79AYD6mNlO51zLWNfhhS/3UAIAACB8fBkorRHtlQUAADje+TJQAgAAIHwIlAAAAPCEQAkAAABPCJQAAADwhEAJAAAAT3wZKLnGGwAAIHx8GSgBAAAQPgRKAAAAeOLLQGmxLgAAAOAE4rtA6YiTAAAAYeW7QAkAAIDwIlACAADAEwIlAAAAPCFQAgAAwBMCJQAAADzxZaDkSTkAAADh48tACSC6cnJylJqaqkAgoNmzZx80v6KiQmPHjlUgENCQIUNUVFRUO2/WrFkKBAJKTU3V8uXLa6dPmjRJnTt3Vt++fffr62c/+5l69eqlfv366bLLLtOWLVsitVoAgBBfBkpjHyUQNVVVVZo6daqWLVum/Px8LVq0SPn5+fu1mT9/vpKSkrRu3TpNmzZN06dPlyTl5+crKytLq1evVk5Ojm666SZVVVVJkq699lrl5OQcNN6wYcP00Ucf6cMPP9Q3vvENzZo1K/IrCQA+57tAyY3NgejKzc1VIBBQz5491bRpU2VmZio7O3u/NtnZ2Zo4caIkafTo0VqxYoWcc8rOzlZmZqYSExPVo0cPBQIB5ebmSpLOO+88tW/f/qDxhg8froSEBEnSWWedpeLi4givIQDAd4ESQHSVlJSoa9eutZ9TUlJUUlLSYJuEhAS1bdtWZWVlR7TsoSxYsEAXX3xxg/PnzZunYDCoYDCo0tLSI+4XALA/AiWAE9Jdd92lhIQEjR8/vsE2U6ZMUV5envLy8tSpU6coVgcAJxYCJYCISk5O1oYNG2o/FxcXKzk5ucE2lZWVKi8vV4cOHY5o2fo8+uijev755/XEE0/IjNNcACDSCJQAImrw4MEqKChQYWGh9uzZo6ysLGVkZOzXJiMjQwsXLpQkLV68WEOHDpWZKSMjQ1lZWaqoqFBhYaEKCgqUnp5+yPFycnL029/+VkuWLFGLFi0itl4AgP8iUAKIqISEBD3wwAMaMWKEevfurTFjxigtLU0zZ87UkiVLJEmTJ09WWVmZAoGA5syZU3trobS0NI0ZM0Z9+vTRyJEjNXfuXMXHx0uSxo0bp7PPPltr165VSkqK5s+fL0m6+eabtW3bNg0bNkwDBgzQDTfcEJsVBwAfMecazy10gsGgy8vLi+gY2287WR+ddKnOuulPER0HwPElGAwq0r8/AFAfM9vpnGsZ6zq8YA8lAAAAPCFQAgAAwBOfBsrGc5gfAADgeOe7QEmUBAAACC/fBUoAAACEF4ESAAAAnhAoAQAA4AmBEgAAAJ4QKAEAAOCJ/wKlxboAAACAE4v/AiUAAADCyqeBkrtRAgAAhIvvAqXjmDcAAEBY+S5QAgAAILwIlAAAAPCEQAkAAABPCJQAAADwxJ+Bkou8gajLyclRamqqAoGAZs+efdD8iooKjR07VoFAQEOGDFFRUVHtvFmzZikQCCg1NVXLly+vnT5p0iR17txZffv23a+vr7/+WsOGDdPpp5+uYcOGafPmzRFbLwCAXwMlgKiqqqrS1KlTtWzZMuXn52vRokXKz8/fr838+fOVlJSkdevWadq0aZo+fbokKT8/X1lZWVq9erVycnJ00003qaqqSpJ07bXXKicn56DxZs+erYsuukgFBQW66KKL6g2wAIDwIVACiLjc3FwFAgH17NlTTZs2VWZmprKzs/drk52drYkTJ0qSRo8erRUrVsg5p+zsbGVmZioxMVE9evRQIBBQbm6uJOm8885T+/btDxqvbl8TJ07Uc889F9kVBACf82mg5Jg3EE0lJSXq2rVr7eeUlBSVlJQ02CYhIUFt27ZVWVnZES17oC+//FKnnHKKJOnkk0/Wl19+Ga5VAQDUIyHWBUQfNzYH/MTMZFb/f/fz5s3TvHnzJEmlpaXRLAsATig+3UMJIJqSk5O1YcOG2s/FxcVKTk5usE1lZaXKy8vVoUOHI1r2QCeddJI+//xzSdLnn3+uzp0719tuypQpysvLU15enjp16nRM6wYAIFACiILBgweroKBAhYWF2rNnj7KyspSRkbFfm4yMDC1cuFCStHjxYg0dOlRmpoyMDGVlZamiokKFhYUqKChQenr6Icer29fChQt16aWXRmbFAACSCJQAoiAhIUEPPPCARowYod69e2vMmDFKS0vTzJkztWTJEknS5MmTVVZWpkAgoDlz5tRemZ2WlqYxY8aoT58+GjlypObOnav4+HhJ0rhx43T22Wdr7dq1SklJ0fz58yVJM2bM0IsvvqjTTz9dL730kmbMmBGbFQcAnzDnGs8FKsFg0OXl5UV0jK23d1F+p1E6a+rDER0HwPElGAwq0r8/AFAfM9vpnGsZ6zq8YA8lAAAAPCFQAjgqf/jDH7R161Y55zR58mQNGjRIL7zwQqzLAgDEEIESwFFZsGCB2rRpoxdeeEGbN2/WY489xjmKAOBzvgyUxo3NgWO277zrpUuXasKECUpLS1NjOhcbABB9vguU/LMHeHPmmWdq+PDhWrp0qUaMGKFt27YpLs53PyUAgDp8+KQcAF7Mnz9fq1atUs+ePdWiRQt9/fXXeuSRR2JdFgAghtitAOCovPXWW0pNTVW7du30+OOP684771Tbtm1jXRYAIIYIlACOyo033qgWLVrogw8+0N13363TTjtN11xzTazLAgDEEIESwFFJSEiQmSk7O1s333yzpk6dqm3btsW6LABADHEOJYCj0rp1a82aNUuPPfaYXn/9dVVXV2vv3r2xLgsAEEO+3EPJld7AsXvqqaeUmJioBQsW6OSTT1ZxcbF+9rOfxbosAEAM+TJQAjh2J598ssaPH6/y8nI9//zzatasGedQAoDP+TJQGjdhBo7Z008/rfT0dD3zzDN6+umnNWTIEC1evDjWZQEAYsh351A6WaxLAI5rd911l95991117txZklRaWqpvf/vbGj16dIwrAwDEii/3UAI4dtXV1bVhUpI6dOig6urqGFYEAIg13+2hBODNyJEjNWLECI0bN05SzUU6o0aNinFVAIBYIlACOCq/+93v9Ne//lVvvvmmJGnKlCm67LLLYlwVACCWOOQN4KhdccUVmjNnjubMmXPEYTInJ0epqakKBAKaPXv2QfMrKio0duxYBQIBDRkyREVFRbXzZs2apUAgoNTUVC1fvvywfa5YsUKDBg3SgAEDdM4552jdunXHvrIAgMMiUAI4Iq1bt1abNm0Oeu2bfihVVVWaOnWqli1bpvz8fC1atEj5+fn7tZk/f76SkpK0bt06TZs2TdOnT5ck5efnKysrS6tXr1ZOTo5uuukmVVVVHbLPG2+8UU888YRWrVqlq666SnfeeWdkvhQAgCSfHvLmpkHA0fPyeMXc3FwFAgH17NlTkpSZmans7Gz16dOntk12drZuv/12SdLo0aN18803yzmn7OxsZWZmKjExUT169FAgEFBubq4kNdinmWnr1q2SpPLycnXp0uWYawcAHJ4vAyWA6CopKVHXrl1rP6ekpOidd95psE1CQoLatm2rsrIylZSU6Kyzztpv2ZKSEklqsM+HH35Yo0aNUvPmzdWmTRu9/fbbEVs3AACHvAGcgO655x4tXbpUxcXFuu666/STn/yk3nbz5s1TMBhUMBhUaWlplKsEgBOHLwOlcdAbiKrk5GRt2LCh9nNxcbGSk5MbbFNZWany8nJ16NChwWUbml5aWqoPPvhAQ4YMkSSNHTtW//rXv+qta8qUKcrLy1NeXp46deoUtvUFAL/xXaDkSTlA9A0ePFgFBQUqLCzUnj17lJWVpYyMjP3aZGRkaOHChZKkxYsXa+jQoTIzZWRkKCsrSxUVFSosLFRBQYHS09Mb7DMpKUnl5eX65JNPJEkvvviievfuHfV1BgA/4RxKABGXkJCgBx54QCNGjFBVVZUmTZqktLQ0zZw5U8FgUBkZGZo8ebImTJigQCCg9u3bKysrS5KUlpamMWPGqE+fPkpISNDcuXMVHx8vSfX2KUl//vOfdcUVVyguLk5JSUlasGBBzNYdAPzAnGs8h3+DwaDLy8uL6Bibb0/RJx2HacjNj0R0HADHl2AwqEj//gBAfcxsp3OuZazr8MJ3h7wBAAAQXgRKAAAAeOLLQNl4DvIDAAAc/3wZKAEAABA+BEoAAAB44stAyZ0oAQAAwseHgZI4CQAAEE4RDZRm1s7MFpvZx2a2xszOjuR4AAAAiL5IPynnD5JynHOjzayppBYRHg8AAABRFrFAaWZtJZ0n6VpJcs7tkbQnUuMBAAAgNiJ5yLuHpFJJj5jZ+2b2sJkd9FghM5tiZnlmlldaWhrBcgAAABAJkQyUCZIGSXrQOTdQ0g5JMw5s5Jyb55wLOueCnTp1imA5AAAAiIRIBspiScXOuXdCnxerJmDGnONZOQAAAGETsUDpnPtC0gYzSw1NukhSfqTGAwAAQGxE+irvH0p6InSF92eSrovweEfEHHsoAQAAwiWigdI5t0pSMJJjHC2iJAAAQHj58Ek5AAAACCcCJQAAADwhUAKIipycHKWmpioQCGj27NkHza+oqNDYsWMVCAQ0ZMgQFRUV1c6bNWuWAoGAUlNTtXz58sP26ZzTL3/5S33jG99Q7969dd9990V03QDA7yJ9UQ4AqKqqSlOnTtWLL76olJQUDR48WBkZGerTp09tm/nz5yspKUnr1q1TVlaWpk+frqeeekr5+fnKysrS6tWrtXHjRn3729/WJ598IkkN9vnoo49qw4YN+vjjjxUXF6evvvoqVqsOAL7AHkoAEZebm6tAIKCePXuqadOmyszMVHZ29n5tsrOzNXHiREnS6NGjtWLFCjnnlJ2drczMTCUmJqpHjx4KBALKzc09ZJ8PPvigZs6cqbi4mp+4zp07R3eFAcBnCJQAIq6kpERdu3at/ZySkqKSkpIG2yQkJKht27YqKytrcNlD9fnpp5/qqaeeUjAY1MUXX6yCgoJ665o3b56CwaCCwaB49CsAHDsCJYATTkVFhZo1a6a8vDx9//vf16RJk+ptN2XKFOXl5SkvL088+hUAjh2BEkDEJScna8OGDbWfi4uLlZyc3GCbyspKlZeXq0OHDg0ue6g+U1JSdPnll0uSLrvsMn344YcRWzcAgA8DpZPFugTAdwYPHqyCggIVFhZqz549ysrKUkZGxn5tMjIytHDhQknS4sWLNXToUJmZMjIylJWVpYqKChUWFqqgoEDp6emH7PN73/ueXn75ZUnSq6++qm984xvRXWEA8Bmu8gYQcQkJCXrggQc0YsQIVVVVadKkSUpLS9PMmTMVDAaVkZGhyZMna8KECQoEAmrfvr2ysrIkSWlpaRozZoz69OmjhIQEzZ07V/Hx8ZJUb5+SNGPGDI0fP1733HOPWrVqpYcffjhm6w4AfmCuET3XOhgMury8vIiOUXb7qVrX4UIN+eHCiI4D4PgSDAYV6d8fAKiPme10zrWMdR1e+O6QNwAAAMKLQAkAAABPCJQAAADwhEAJAAAATwiUAAAA8MSngbLxXNkOAABwvPNdoOTG5gAAAOHlu0AJAACA8CJQAgAAwBMCJQAAADwhUAIAAMATAiUAAAA88Weg5K5BAAAAYeO7QMlNgwAAAMLLd4ESAAAA4eXTQMkxbwAAgHDxXaAkSgIAAISX7wIlgNjIyclRamqqAoGAZs+efdD8iooKjR07VoFAQEOGDFFRUVHtvFmzZikQCCg1NVXLly8/4j5/9KMfqVWrVhFZHwDAfxEoAURcVVWVpk6dqmXLlik/P1+LFi1Sfn7+fm3mz5+vpKQkrVu3TtOmTdP06dMlSfn5+crKytLq1auVk5Ojm266SVVVVYftMy8vT5s3b47qegKAXxEoAURcbm6uAoGAevbsqaZNmyozM1PZ2dn7tcnOztbEiRMlSaNHj9aKFSvknFN2drYyMzOVmJioHj16KBAIKDc395B9VlVV6Wc/+5l++9vfRn1dAcCPCJQAIq6kpERdu3at/ZySkqKSkpIG2yQkJKht27YqKytrcNlD9fnAAw8oIyNDp5xySiRXCwAQkhDrAgAgnDZu3KhnnnlGr7zyymHbzps3T/PmzZMklZaWRrgyADhxsYcSQMQlJydrw4YNtZ+Li4uVnJzcYJvKykqVl5erQ4cODS7b0PT3339f69atUyAQUPfu3bVz504FAoF665oyZYry8vKUl5enTp06hXOVAcBXCJQAIm7w4MEqKChQYWGh9uzZo6ysLGVkZOzXJiMjQwsXLpQkLV68WEOHDpWZKSMjQ1lZWaqoqFBhYaEKCgqUnp7eYJ+XXHKJvvjiCxUVFamoqEgtWrTQunXrYrHaAOAbvjzkbdyNEoiqhIQEPfDAAxoxYoSqqqo0adIkpaWlaebMmQoGg8rIyNDkyZM1YcIEBQIBtW/fXllZWZKktLQ0jRkzRn369FFCQoLmzp2r+Ph4Saq3TwBA9JlzjSdcBYNBl5eXF9ExSm/vrsL25yj9R49HdBwAx5dgMKhI//4AQH3MbKdzrmWs6/CCQ94AAADwhEAJAAAATwiUAAAA8IRACQAAAE8IlAAAAPDEl4GyEV3YDgAAcNzzZaAEAABA+PgyUHJjcwAAgPDxXaB0sliXAAAAcELxXaAEAABAeBEoAQAA4AmBEgAAAJ4QKAEAAOAJgRIAAACeECgBAADgCYESAAAAnvg0UHJjcwAAgHDxaaAEAABAuBAoAURFTk6OUlNTFQgENHv27IPmV1RUaOzYsQoEAhoyZIiKiopq582aNUuBQECpqalavnz5YfscP368UlNT1bdvX02aNEl79+6N6LoBgN8RKAFEXFVVlaZOnaply5YpPz9fixYtUn5+/n5t5s+fr6SkJK1bt07Tpk3T9OnTJUn5+fnKysrS6tWrlZOTo5tuuklVVVWH7HP8+PH6+OOP9e9//1u7du3Sww8/HPV1BoBwMrORZrbWzNaZ2Yx65iea2VOh+e+YWfc6834emr7WzEYcrk8zeyI0/SMzW2BmTQ5XH4ESQMTl5uYqEAioZ8+eatq0qTIzM5Wdnb1fm+zsbE2cOFGSNHr0aK1YsULOOWVnZyszM1OJiYnq0aOHAoGAcnNzD9nnqFGjZGYyM6Wnp6u4uDjq6wwA4WJm8ZLmSrpYUh9J48yszwHNJkva7JwLSLpH0v+Flu0jKVNSmqSRkv5oZvGH6fMJSb0knSGpuaTrD1cjgRJAxJWUlKhr1661n1NSUlRSUtJgm4SEBLVt21ZlZWUNLnskfe7du1ePPfaYRo4cWW9d8+bNUzAYVDAYVGlpqef1BIAISZe0zjn3mXNuj6QsSZce0OZSSQtD7xdLusjMLDQ9yzlX4ZwrlLQu1F+DfTrnlroQSbmSUg5XIIESwAnrpptu0nnnnadzzz233vlTpkxRXl6e8vLy1KlTpyhXBwBHLFnShjqfi0PT6m3jnKuUVC6pwyGWPWyfoUPdEyTlHK7AhCNYCQDwJDk5WRs2/Pd3q7i4WMnJyfW2SUlJUWVlpcrLy9WhQ4dDLnuoPu+44w6VlpbqT3/6U6RWCwBOdH+U9Jpz7vXDNWQPJYCIGzx4sAoKClRYWKg9e/YoKytLGRkZ+7XJyMjQwoU1R2sWL16soUOHysyUkZGhrKwsVVRUqLCwUAUFBUpPTz9knw8//LCWL1+uRYsWKS6OnzkAx70SSV3rfE4JTau3jZklSGorqewQyx6yTzO7TVInST85kgL9uYfScWNzIJoSEhL0wAMPaMSIEaqqqtKkSZOUlpammTNnKhgMKiMjQ5MnT9aECRMUCATUvn17ZWVlSZLS0tI0ZswY9enTRwkJCZo7d67i4+Mlqd4+JemGG25Qt27ddPbZZ0uSLr/8cs2cOTM2Kw8A3r0r6XQz66Ga0Jcp6aoD2iyRNFHSW5JGS/qnc86Z2RJJT5rZHEldJJ2umvMiraE+zex6SSMkXeScqz6SAs01onAVDAZdXl5eRMf48vaeWt9uiNJvWRTRcQAcX4LBoI7q92fZDOmLf0euIADHn5PPkC4++D67h2NmO51zLQ/TZpSkeyXFS1rgnLvLzH4tKc85t8TMmkl6TNJASV9LynTOfRZa9peSJkmqlHSLc25ZQ32GpldKWi9pW2j4Z51zvz5Uff7cQwkAAHAccc4tlbT0gGkz67zfLenKBpa9S9JdR9JnaPpR50MCJQAci2PYCwEAJyrOVgcAAIAnRxQozezHZtbGasw3s/fMbHikiwMAAEDjd6R7KCc557ZKGi4pSTU3ueR4DwAAAI44UFro7yhJjznnVteZBgAAAB870kC50sxeUE2gXG5mrSUd0X2JAAAAcGI70qu8J0saIOkz59xOM2sv6bqIVQUAAIDjxpHuoTxb0lrn3BYzu1rSr1Tz0HEAAAD43JEGygcl7TSz/pJ+KulTSX+JWFUR5Dj1EwAAIKyO9JB3Zeh5kJdKesA5N9/MJkeyMACIpqKiIgWDwaNaprS0VJ06dYpQRY1/fGpoPDXEenxq8Dx+03DXEm1HGii3mdnPVXO7oHPNLE5Sk8iVBQDRtWnTpqNe5qif/x1msR6fGhpPDbEenxq8jW9mH0SgnKg60kPeYyVVqOZ+lF9ISpH0u4hVBQAAgOPGEQXKUIh8QlJbM/uOpN3OuePyHEoAAACE15E+enGMpFxJV0oaI+kdMxsdycIAoLGbMmWKr8enhsZTQ6zHp4bGMX4smXPu8I1qju0Pc859FfrcSdJLzrn+4SwmGAy6SJ/78MXtp+k/7dKVfsuiiI4DAABwJMxspXPu6K4KbGSO9BzKuH1hMqTsKJYFAADACexIQ2GOmS03s2vN7FpJ/5C0NHJlRZbp8HtlAfhTVVWVBg4cqO985zuSpH/+858aNGiQ+vbtq4kTJ6qyslKStHnzZl122WXq16+f0tPT9dFHH9X2MWnSJHXu3Fl9+/aNSQ0bNmzQhRdeqD59+igtLU1/+MMfol7D7t27lZ6erv79+ystLU233XZb1GtoqJ9ojt+9e3edccYZGjBgwFHflipcNWzZskWjR49Wr1691Lt3b7311ltRrWHt2rUaMGBA7atNmza69957o/493HPPPUpLS1Pfvn01btw47d69O6rj/+EPf1Dfvn2VlpZ21Ot/XHDOHdFL0hWS5oRelx3pckfzOvPMM12kfX7baS73nrERHwfA8enuu+9248aNc5dccomrqqpyKSkpbu3atc455/73f//XPfzww84552699VZ3++23O+ecW7NmjRs6dGhtH6+++qpbuXKlS0tLi0kNGzdudCtXrnTOObd161Z3+umnu9WrV0e1hurqardt2zbnnHN79uxx6enp7q233opqDfX1E+3xu3Xr5kpLS49q3HDXcM0117g///nPzjnnKioq3ObNm6Newz6VlZXupJNOckVFRVGtobi42HXv3t3t3LnTOefclVde6R555JGojf/vf//bpaWluR07dri9e/e6iy66yBUUFNT2LynPRSBXRfN1xIetnXN/dc79JPT6W2TiLQDETnFxsf7xj3/o+uuvlySVlZWpadOm+sY3viFJGjZsmP76179KkvLz8zV06FBJUq9evVRUVKQvv/xSknTeeeepffv2MavhlFNO0aBBgyRJrVu3Vu/evVVSUhLVGsxMrVq1kiTt3btXe/fuldmRP6ksXNviwH6iPb4X4aihvLxcr732miZPrnkWSdOmTdWuXbuo1lDXihUrdNppp6lbt25Rr6GyslK7du1SZWWldu7cqS5dukRt/DVr1mjIkCFq0aKFEhISdP755+vZZ5894u/geHDIQGlm28xsaz2vbWa2NVpFAkA03HLLLfrtb3+ruLian8aOHTuqsrKy9kbFixcv1oYNGyRJ/fv3r/0HITc3V+vXr1dxcXGjq6GoqEjvv/++hgwZEvUaqqqqNGDAAHXu3FnDhg2LSQ0H9hPt8c1Mw4cP15lnnql58+ZFvYbCwkJ16tRJ1113nQYOHKjrr79eO3bsiPr3sE9WVpbGjRsX9e8hOTlZt956q0499VSdcsopatu2rYYPHx618fv27avXX39dZWVl2rlzp5YuXVq7zInikP+FOedaO+fa1PNq7ZxrE60iASDSnn/+eXXu3Flnnnlm7TQzU1ZWlqZNm6b09HS1bt1a8fHxkqQZM2Zoy5YtGjBggO6//34NHDiwdl5jqWH79u264oordO+996pNmyP7yQ5nDfHx8Vq1apWKi4uVm5t70LmNka6hvn6i/R288cYbeu+997Rs2TLNnTtXr732WlRrqKys1Hvvvacbb7xR77//vlq2bKnZs2dH/XuQpD179mjJkiW68sorj2j8cNawefNmZWdnq7CwUBs3btSOHTv0+OOPR2383r17a/r06Ro+fLhGjhypAQMGeP69aHRifcy97otzKAHEyowZM1xycrLr1q2bO+mkk1zz5s3d+PHj92uzfPlyd+WVVx60bHV1tevWrZsrLy+vnVZYWHjU51CGs4Y9e/a44cOHu7vvvjtmNdR1xx13uN/97ndRreFI+onk+Ae67bbbov4dfP75565bt26181577TU3atSoqNawz3PPPeeGDRt2RGOHu4ann37aTZo0qXbewoUL3Y033hi18Q/085//3M2dO7f2s06AcyhjXkDdF4ESQGPw8ssv117A8eWXXzrnnNu9e7cbOnSoW7FihXPOuc2bN7uKigrnnHPz5s1zEyZM2K+PYwmU4aqhurraTZgwwf34xz8+5vG91vDVV1/VXvyxc+dOd84557i///3vUa2hoX6iNf727dvd1q1ba9+fffbZbtmyZVGtwTnnzjnnHPfxxx8752pC7a233hr1GpxzbuzYsW7BggVHPXY4anj77bddnz593I4dO1x1dbW75ppr3H333Re18esus379epeamrrfxVEnQqBMiPUe0lhw3DUIwBH63e9+p+eff17V1dW68cYba0+4X7NmjSZOnCgzU1pamubPn1+7zLhx4/TKK69o06ZNSklJ0R133FF7UUQ0anjzzTf12GOP1d6uRpJ+85vfaNSoUVGr4fPPP9fEiRNVVVWl6upqjRkz5qhv2+O1hnA72vG//PJLXXbZZZJqLgi56qqrNHLkyKjWIEn333+/xo8frz179qhnz5565JFHol7Djh079OKLL+pPf/qTp7GPtYYhQ4Zo9OjRGjRokBISEjRw4EBPT7U5lu/giiuuUFlZmZo0aaK5c+ce1cVRx4MjelJOtETnSTkB/adtUOnTsiI6DgAAwJHw05NyTijc2BwAACB8fBcoiZIAAADh5btACQAAgPAiUAIAAMATAiUAAAA8IVACABABr7zyiudbJQHHCwIlAAAAPPFloORKbwDAPo8//rjS09M1YMAA/eAHP1BVVZVatWqladOmKS0tTRdddJFKS0slSatWrdJZZ52lfv366bLLLtPmzZslSevWrdO3v/1t9e/fX4MGDdKnn34qqeZ56qNHj1avXr00fvx47bv384wZM9SnTx/169dPt956a2xWHAgj/wVKi3UBAIDGYs2aNXrqqaf05ptvatWqVYqPj9cTTzyhHTt2KBgMavXq1Tr//PN1xx13SJKuueYa/d///Z8+/PBDnXHGGbXTx48fr6lTp+qDDz7Qv/71L51yyimSpPfff1/33nuv8vPz9dlnn+nNN99UWVmZ/va3v2n16tX68MMP9atf/Spm6w+Ei/8CpbixOQCgxooVK7Ry5UoNHjxYAwYM0IoVK/TZZ58pLi5OY8eOlSRdffXVeuONN1ReXq4tW7bo/PPPlyRNnDhRr732mrZt26aSkpLaxyw2a9ZMLVq0kCSlp6crJSVFcXFxGjBggIqKitS2bVs1a9ZMkydP1rPPPlvbFjie+S5QOnZRAgBCnHOaOHGiVq1apVWrVmnt2rW6/fbbD2pndmz/diQmJta+j4+PV2VlpRISEpSbm6vRo0fr+eef9/x8b6Ax8F2gBABgn4suukiLFy/WV199JUn6+uuvtX79elVXV2vx4sWSpCeffFLnnHOO2rZtq6SkJL3++uuSpMcee0znn3++WrdurZSUFD333HOSpIqKCu3cubPBMbdv367y8nKNGjVK99xzjz744IPIriQQBQmxLgAAgFjp06eP7rzzTg0fPlzV1dVq0qSJ5s6dq5YtWyo3N1d33nmnOnfurKeeekqStHDhQt1www3auXOnevbsqUceeURSTbj8wQ9+oJkzZ6pJkyZ65plnGhxz27ZtuvTSS7V792455zRnzpyorCsQSbbvirPGIBgMury8vIiOsfGO01XSZqAGT3s6ouMAAI5frVq10vbt22NdBnzCzFY654KxrsMLDnkDAADAk4gHSjOLN7P3zez5SI8FAEA4sHcSODrR2EP5Y0lrojAOAAAAYiCigdLMUiRdIunhSI4DAACA2In0Hsp7Jf2PpOqGGpjZFDPLM7O8fY+2irzGcyESAADA8S5igdLMviPpK+fcykO1c87Nc84FnXPBTp06RaqcupVFYQwAAAD/iOQeym9JyjCzIklZkoaa2eMRHA8AAAAxELFA6Zz7uXMuxTnXXVKmpH86566O1HgAAACIDe5DCQAAAE+i8uhF59wrkl6JxlgAAACILvZQAgAAwBMCJQAAADwhUAIAAMATAiUAAAA88WWgNMeTcgAAAMLFd4GSKAkAABBevguUAAAACC8CJQAAADwhUAIAAMATAiUAAAA8IVACAADAE18GSq70BgAACB9fBkoAAACEjy8DpbGPEgAAIGx8FyidLNYlAAAAnFB8FygBAAAQXgRKAAAAeEKgBAAAgCcESgAAAHhCoAQAAIAnBEoAAAB44rtAyU2DAAAAwst3gRIAAADh5btAyY3NAQAAwst3gRIAAADhRaAEAACAJwRKAAAAeEKgBAAAgCcESgAAAHjiz0DpYl0AAADAicOfgRIAAABh49NAyS5KAACAcPFdoOTG5gAAAOHlu0AJAACA8CJQAgAAwBMCJQAAADwhUAIAAMATAiUAAAA8IVACAADAEwIlAAAAPPFloDRubA4AABA2vgyUAAAACB8CJQAAADwhUAIAAMATAiUAAAA8IVACAADAEwIlAAAAPCFQAgAAwBNfBkruQgkAABA+vgyUAAAACB9fBkqelAMAABA+vguUzizWJQAAAJxQfBcoAQAAEF4ESgAAAHhCoAQAAIAnBEoAAAB4QqAEAACAJ74MlNw0CAAAIHx8GSgBAAAQPr4MlObYRwkAABAuvguUTtzYHAAAIJx8FygBAAAQXgRKAAAAeEKgBAAAgCcESgAAAHhCoAQAAIAnBEoAAAB4QqAEAACAJ74MlMbDFwEAAMLGh4GSG5sDAACEkw8DJQAAAMKJQAkAAABPCJQAAADwhEAJAAAATwiUAAAA8IRACQAAAE98GSi5CyUAAED4+DJQcmNzAACA8PFdoCRKAgAAhJfvAiUAAADCi0AJAAAATwiUAAAA8IRACQAAAE8IlAAAAPCEQAkAAABPfBkoHfcOAgAACBtfBkqLdQEAAAAnEB8GSuIkAABAOPkwUAIAACCcCJQAAADwhEAJAAAATwiUAAAA8IRACQAAAE8IlAAAAPCEQAkAAABPCJQAAADwxKeBkmcvAgAAhIvvAqXjSTkAAABh5btACQAAgPAiUAIAAMATAiUAAAA8IVACAADAEwIlAAAAPCFQAgAAwJOIBUoz62pmL5tZvpmtNrMfR2osAAAAxE5CBPuulPRT59x7ZtZa0koze9E5lx/BMY+IcWNzAACAsInYHkrn3OfOufdC77dJWiMpOVLjHTlubA4AABBOUTmH0sy6Sxoo6Z1ojAcAAIDoiXigNLNWkv4q6Rbn3NZ65k8xszwzyystLY10OQAAAAiziAZKM2uimjD5hHPu2fraOOfmOeeCzrlgp06dIlkOAAAAIiCSV3mbpPmS1jjn5kRqHAAAAMRWJPdQfkvSBElDzWxV6DUqguMBAAAgBiJ22yDn3BvikmoAAIATni+flOO4DSUAAEDY+DJQcmNzAACA8PFdoCRKAgAAhJfvAiUAAADCi0AJAAAATwiUAAAA8IRACQAAAE8IlAAAAPCEQAkAAABPCJQAAADwxH+B0iTuRgkAABA+vguUjseLAwAAhJXvAiUAAADCi0AJAAAATwiUAAAA8IRACQAAAE8IlAAAAPCEQAkAAABPCJQAAADwhEAJAAAAT3wYKLmxOQAAQDj5MFACAAAgnAiUAAAA8IRACQAAAE8IlAAAAPCEQAkAAABPCJQAAADwxKeB0sW6AAAAgBOGTwMlAAAAwsWXgZJbmwMAAISP7wKlI04CAACEle8CJQAAAMKLQAkAAABPCJQAAADwhEAJAAAATwiUAAAA8IRACQAAAE/8GSh5UA4AAEDY+DNQkigBAADCxqeBEgAAAOFCoAQAAIAnBEoAAAB4QqAEAACAJwRKAAAAeEKgBAAAgCcESgAAAHhCoAQAAIAnvgyUxo3NAQAAwsZ3gdLJYl0CAADACcV3gRIAAADhRaAEAACAJwRKAAAAeEKgBAAAgCcESgAAAHhCoAQAAIAnvgyU3IUSAAAgfPwXKI0bmwMAAIST/wIlNzYHAAAIKx8GSgAAAIQTgRIAAACeECgBAADgCYESAAAAnhAoAQAA4AmBEgAAAJ4QKAEAAOCJPwOl48bmAAAA4eK7QOm4sTkAAEBY+S5QAgAAILwIlAAAAPCEQAkAAABPCJQAAADwhEAJAAAATwiUAAAA8IRACQAAAE8IlAAAAPDEl4HSxJNyAAAAwsV3gdJZHI9eBAAACCP/BUrFyVQd6zIAAABOGD4MlCZjDyUAAEDY+C9QWpzEHkoAAICw8V+glCnOESgBAADCxXeBUhYncZU3AABA2PguUDoZF+UAAACEkf8CpcVxUQ4AAEAY+S9QctsgAACAsPJfoDRTHHsoAQAAwsZ3gVLsoQQAAAgr3wVKZ3E8yxsAACCMfBgoTcZ9KAEAAMLGd4Gy5pA3eygBAADCxXeBsuaQN3soAQAAwsV/gVLGHkoAAIAw8l2glMVxDiUAAEAY+S5QOotTHHsoAQAAwsZ3gVI8yxsAACCsfBcouQ8lAABAePkuUMriFMc5lAAAAGHju0DJVd4AAADh5btAKQ55AwAAhJXvAmXNVd4c8gYAAAgX3wVK9lACAACEly8DJfehBAAACB/fBUqX0Fwt3C5VV1XFuhQAAIATgu8Cpdr3VKLt1Zo/jtPG95ZKVZWxrggAAOC4lhDrAqLt9G9P0iuFbyu46VW1WrJcW5a00bq2Z0unnqWkXueq6zcGqmkT330tAAAAx8yci9z5hGY2UtIfJMVLetg5N/tQ7YPBoMvLy4tYPXV9Xva11rz+nFqt+7sC2/PUXlslSTtcokoSumpzy9O0q93pSuh4mlp17q4Oyaep88nJSiRsAgCAMDKzlc65YKzr8CJigdLM4iV9ImmYpGJJ70oa55zLb2iZaAbKuqqrqvWfdf/W1x+/rurPP1TzLQU6qaJQHd3m/drtdk30hXXU1rgk7WrSTnsSk1TdvIOqm3eQa9FRCc3bqknLtmraoo2atWqrFq3aqUXrdmrWvKUSm8QrMSFOZhb19QMAAI3XiRAoI7m7LV3SOufcZ5JkZlmSLpXUYKCMlbj4OHVP7a/uqf33m165/WttKi7Qli8Ktau0UG5LsZru2KhmFV8raW+JWm1brdZbtynhMPe1rHRx2qFm2qJE7VET7bWm2mtNVWlNVGmJqoxrquq4pqqMa6qq0MtZglxcghQXL2cJUlyCFB8vWYIsLl4uPkEWV/NSXHxNW0uQ4uLq/LWaAGvxMlPor4Ve++bHq+ZtXM0tlfZNs9C0uDhJppoJ+97XnHrr7L8B+b85uc5pubbfn9DsuDrTTPvn67j9+rK6S9r+byzODphltQvW/LH9F6vzzh2U6Q+YcFDotwbeNyKNtKzw88eK+un/d/poVXEcOi3QS82aNYt1GceFSAbKZEkb6nwuljQkguOFXUKr9jq51xCd3OsQZVdXq3rXFu3c8oV2bdui3dvLVbGzXHt2blXlrq2q3rVV2rNdcXu3y/bulqoqFFdVIauqUELVHiVWVyi+epviqyqUULlXTdweNXV7FK8qxbkqxala8apSvKoVzw3ZAQCImg1Xv6GugTNiXcZxIeYnBJrZFElTJOnUU0+NcTXHIC5OcS3bq1XL9moV6bGqqyVXJVVXhl5Vqq6qVGXlXrnqvVJVlaqr9qq6ulKu2slVV8u5ajnnVF1dJedqplWHprmq0N/qKlW7mrDq9mtX81eqlpyTOaeaUySczNX0XVfd0yecQu0bmG9y2v9sCxdqs6/tgeG5bntXd3JotDqfQw33Tdv/RvYHnOJx0CkfDX+2CJ5v7EXjrCr8/LKe/llRX60qjlPdT0qOdQnHjUgGyhJJXet8TglN249zbp6keVLNOZQRrOf4FxcnKU6Kb/LfSZKaxqwgAACAyN6H8l1Jp5tZDzNrKilT0pIIjgcAAIAYiNgeSudcpZndLGm5am4btMA5tzpS4wEAACA2InoOpXNuqaSlkRwDAAAAseW/Ry8CAAAgrAiUAAAA8IRACQAAAE8IlAAAAPCEQAkAAABPCJQAAADwhEAJAAAATwiUAAAA8IRACQAAAE8IlAAAAPCEQAkAAABPCJQAAADwhEAJAAAATwiUAAAA8IRACQAAAE8IlAAAAPCEQAkAAABPCJQAAADwhEAJAAAATwiUAAAA8IRACQAAAE/MORfrGmqZWamk9VEYqqOkTVEYB8eObdS4sX0aP7ZR48c2atyiuX26Oec6RWmsiGhUgTJazCzPOReMdR1oGNuocWP7NH5so8aPbdS4sX2ODoe8AQAA4AmBEgAAAJ74NVDOi3UBOCy2UePG9mn82EaNH9uocWP7HAVfnkMJAACA8PHrHkoAAACEia8CpZmNNLO1ZrbOzGbEuh4/MbMFZvaVmX1UZ1p7M3vRzApCf5NC083M7gttpw/NbFCdZSaG2heY2cRYrMuJyMy6mtnLZpZvZqvN7Meh6WyjRsLMmplZrpl9ENpGd4Sm9zCzd0Lb4ikzaxqanhj6vC40v3udvn4emr7WzEbEaJVOWGYWb2bvm9nzoc9so0bEzIrM7N9mtsrM8kLT+K3zyjnni5ekeEmfSuopqamkDyT1iXVdfnlJOk/SIEkf1Zn2W0kzQu9nSPq/0PtRkpZJMklnSXonNL29pM9Cf5NC75NivW4nwkvSKZIGhd63lvSJpD5so8bzCn3XrULvm0h6J/TdPy0pMzT9IUk3ht7fJOmh0PtMSU+F3vcJ/f4lSuoR+l2Mj/X6nUgvST+R9KSk50Of2UaN6CWpSFLHA6bxW+fx5ac9lOmS1jnnPnPO7ZGUJenSGNfkG8651yR9fcDkSyUtDL1fKOl7dab/xdV4W1I7MztF0ghJLzrnvnbObZb0oqSRES/eB5xznzvn3gu93yZpjaRksY0ajdB3vT30sUno5SQNlbQ4NP3AbbRv2y2WdJGZWWh6lnOuwjlXKGmdan4fEQZmliLpEkkPhz6b2EbHA37rPPJToEyWtKHO5+LQNMTOSc65z0Pvv5B0Uuh9Q9uKbRgFocNuA1WzB4xt1IiEDqWukvSVav4B+1TSFudcZahJ3e+7dluE5pdL6iC2UaTdK+l/JFWHPncQ26ixcZJeMLOVZjYlNI3fOo8SYl0AINXsfTEzbjkQY2bWStJfJd3inNtas7OkBtso9pxzVZIGmFk7SX+T1Cu2FaEuM/uOpK+ccyvN7IIYl4OGneOcKzGzzpJeNLOP687kt+7Y+GkPZYmkrnU+p4SmIXa+DB06UOjvV6HpDW0rtmEEmVkT1YTJJ5xzz4Yms40aIefcFkkvSzpbNYfg9u0cqPt9126L0Py2ksrENoqkb0nKMLMi1ZxWNVTSH8Q2alSccyWhv1+p5v+YpYvfOs/8FCjflXR66Gq7pqo5AXpJjGvyuyWS9l0ZN1FSdp3p14SurjtLUnnoUMRyScPNLCl0Bd7w0DR4FDpva76kNc65OXVmsY0aCTPrFNozKTNrLmmYas51fVnS6FCzA7fRvm03WtI/Xc3VBEskZYauMO4h6XRJuVFZiROcc+7nzrkU51x31fwb80/n3HixjRoNM2tpZq33vVfNb9RH4rfOu1hfFRTNl2qu1vpENecd/TLW9fjpJWmRpM8l7VXNuSaTVXOu0ApJBZJektQ+1NYkzQ1tp39LCtbpZ5JqTlBfJ+m6WK/XifKSdI5qziv6UNKq0GsU26jxvCT1k/R+aBt9JGlmaHpP1YSNdZKekZQYmt4s9HldaH7POn39MrTt1kq6ONbrdiK+JF2g/17lzTZqJK/Qtvgg9Fq9LwvwW+f9xZNyAAAA4ImfDnkDAAAgAgiUAAAA8IRACQAAAE8IlAAAAPCEQAkAAABPCJQA0AAzu8DMno91HQDQ2BEoAQAA4AmBEsBxz8yuNrNcM1tlZn8ys3gz225m95jZajNbYWadQm0HmNnbZvahmf0t9JQLmVnAzF4ysw/M7D0zOy3UfSszW2xmH5vZE6GnCsnMZptZfqif38do1QGgUSBQAjiumVlvSWMlfcs5N0BSlaTxklpKynPOpUl6VdJtoUX+Imm6c66fap58sW/6E5LmOuf6S/qmap7sJEkDJd0iqY9qnrLxLTPrIOkySWmhfu6M5DoCQGNHoARwvLtI0pmS3jWzVaHPPSVVS3oq1OZxSeeYWVtJ7Zxzr4amL5R0XujZvsnOub9JknNut3NuZ6hNrnOu2DlXrZpHUnaXVC5pt6T5Zna5pH1tAcCXCJQAjncmaaFzbkDoleqcu72edsf6nNmKOu+rJCU45yolpUtaLOk7knKOsW8AOCEQKAEc71ZIGm1mnSXJzNqbWTfV/L6NDrW5StIbzrlySZvN7NzQ9AmSXnXObZNUbGbfC/WRaGYtGhrQzFpJauucWyppmqT+EVgvADhuJMS6AADwwjmXb2a/kvSCmcVJ2itpqqQdktJD875SzXmWkjRR0kOhwPiZpOtC0ydI+pOZ/TrUx5WHGLa1pGwza6aaPaQ/CfNqAcBxxZw71qNAANB4mdl251yrWNcBAH7AIW8AAAB4wh5KAAAAeMIeSgAAAHhCoAQAAIAnBEoAAAB4QqAEAACAJwRKAAAAeEKgBAAAgCf/HzRcNFIJMgxIAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "epochs = 5000\n",
        "alfa = 0.1\n",
        "p_degree = 1\n",
        "lambd = 0.01\n",
        "regularizer = \"ridge\"\n",
        "ratio = 0\n",
        "\n",
        "W, loss_train, loss_val, X_test_pow, r2_train, r2_val = train_output_print(X_train, Y_train, X_validation, Y_validation, X_test, Y_test, epochs, alfa, p_degree, lambd, regularizer, ratio)\n",
        "loss_test, r2_test = test_output_print(X_test_pow, W, Y_test, lambd, regularizer, ratio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prueba hiperparámetros lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(columns=[\"regularizer\",\"lambd\",\"alfa\",\"epochs\",\"p_degree\",\"ratio\", \"loss_train\", \"loss_val\", \"loss_test\", \"r2_train\", \"r2_val\", \"r2_test\"])\n",
        "for r in [\"lasso\"]:\n",
        "  for e in [100, 1000, 5000]:\n",
        "    for l in [0.001, 0.01, 0.1, 1, 5]:\n",
        "      for a in [0.1, 0.01, 0.001, 0.0001]:\n",
        "        for p in [1,2,4,6,8,10,20]:\n",
        "          if r == \"elasticnet\":\n",
        "\n",
        "            for ra in [0.3, 0.5, 0.7, 0.9]:\n",
        "              print(\"---\\nregularizer:\",r,\"lambd:\",l,\"alfa:\",a,\"epochs:\",e,\"p_degree:\", p, \"ratio:\", ra)\n",
        "\n",
        "              epochs = e \n",
        "              alfa = a \n",
        "              p_degree = p \n",
        "              lambd = l \n",
        "              regularizer = r\n",
        "              ratio = ra\n",
        "              W, loss_train, loss_val, X_test_pow, r2_train, r2_val = train_output_print(X_train, Y_train, X_validation, Y_validation, X_test, Y_test, epochs, alfa, p_degree, lambd, regularizer, ratio)\n",
        "              loss_test, r2_test = test_output_print(X_test_pow, W, Y_test, lambd, regularizer, ratio)\n",
        "\n",
        "              df = df.append({\"regularizer\":r,\"lambd\":l,\"alfa\":a,\"epochs\":e,\"p_degree\":p,\"ratio\":ra, \"loss_train\":loss_train[-1], \"loss_val\":loss_val[-1], \"loss_test\":loss_test, \"r2_train\":r2_train, \"r2_val\":r2_val, \"r2_test\":r2_test}, ignore_index=True)\n",
        "          else:\n",
        "            print(\"---\\nregularizer:\",r,\"lambd:\",l,\"alfa:\",a,\"epochs:\",e,\"p_degree:\",p)\n",
        "            epochs = e \n",
        "            alfa = a \n",
        "            p_degree = p \n",
        "            lambd = l \n",
        "            regularizer = r \n",
        "            ratio = 0.3 \n",
        "            W, loss_train, loss_val, X_test_pow, r2_train, r2_val = train_output_print(X_train, Y_train, X_validation, Y_validation, X_test, Y_test, epochs, alfa, p_degree, lambd, regularizer, ratio)\n",
        "            loss_test, r2_test = test_output_print(X_test_pow, W, Y_test, lambd, regularizer, ratio)\n",
        "            df = df.append({\"regularizer\":r,\"lambd\":l,\"alfa\":a,\"epochs\":e,\"p_degree\":p,\"ratio\":0, \"loss_train\":loss_train[-1], \"loss_val\":loss_val[-1], \"loss_test\":loss_test, \"r2_train\":r2_train, \"r2_val\":r2_val, \"r2_test\":r2_test}, ignore_index=True)\n",
        "\n",
        "df.to_csv(\"lasso_tests.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lasso testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 5000\n",
        "alfa = 0.1\n",
        "p_degree = 1\n",
        "lambd = 0.001\n",
        "regularizer = \"lasso\"\n",
        "ratio = 0\n",
        "\n",
        "W, loss_train, loss_val, X_test_pow, r2_train, r2_val = train_output_print(X_train, Y_train, X_validation, Y_validation, X_test, Y_test, epochs, alfa, p_degree, lambd, regularizer, ratio)\n",
        "loss_test, r2_test = test_output_print(X_test_pow, W, Y_test, lambd, regularizer, ratio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prueba hiperparámetros elasticnet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(columns=[\"regularizer\",\"lambd\",\"alfa\",\"epochs\",\"p_degree\",\"ratio\", \"loss_train\", \"loss_val\", \"loss_test\", \"r2_train\", \"r2_val\", \"r2_test\"])\n",
        "for r in [\"elasticnet\"]:\n",
        "  for e in [100, 1000, 5000]:\n",
        "    for l in [0.001, 0.01, 0.1, 1, 5]:\n",
        "      for a in [0.1, 0.01, 0.001, 0.0001]:\n",
        "        for p in [1,2,4,6,8,10,20]:\n",
        "          if r == \"elasticnet\":\n",
        "            for ra in [0.3, 0.5, 0.7, 0.9]:\n",
        "              print(\"---\\nregularizer:\",r,\"lambd:\",l,\"alfa:\",a,\"epochs:\",e,\"p_degree:\", p, \"ratio:\", ra)\n",
        "\n",
        "              epochs = e \n",
        "              alfa = a \n",
        "              p_degree = p \n",
        "              lambd = l \n",
        "              regularizer = r \n",
        "              ratio = ra\n",
        "              W, loss_train, loss_val, X_test_pow, r2_train, r2_val = train_output_print(X_train, Y_train, X_validation, Y_validation, X_test, Y_test, epochs, alfa, p_degree, lambd, regularizer, ratio)\n",
        "              loss_test, r2_test = test_output_print(X_test_pow, W, Y_test, lambd, regularizer, ratio)\n",
        "\n",
        "              df = df.append({\"regularizer\":r,\"lambd\":l,\"alfa\":a,\"epochs\":e,\"p_degree\":p,\"ratio\":ra, \"loss_train\":loss_train[-1], \"loss_val\":loss_val[-1], \"loss_test\":loss_test, \"r2_train\":r2_train, \"r2_val\":r2_val, \"r2_test\":r2_test}, ignore_index=True)\n",
        "          else:\n",
        "            print(\"---\\nregularizer:\",r,\"lambd:\",l,\"alfa:\",a,\"epochs:\",e,\"p_degree:\",p)\n",
        "            epochs = e \n",
        "            alfa = a \n",
        "            p_degree = p \n",
        "            lambd = l \n",
        "            regularizer = r \n",
        "            ratio = 0.3 \n",
        "            W, loss_train, loss_val, X_test_pow, r2_train, r2_val = train_output_print(X_train, Y_train, X_validation, Y_validation, X_test, Y_test, epochs, alfa, p_degree, lambd, regularizer, ratio)\n",
        "            loss_test, r2_test = test_output_print(X_test_pow, W, Y_test, lambd, regularizer, ratio)\n",
        "            df = df.append({\"regularizer\":r,\"lambd\":l,\"alfa\":a,\"epochs\":e,\"p_degree\":p,\"ratio\":0, \"loss_train\":loss_train[-1], \"loss_val\":loss_val[-1], \"loss_test\":loss_test, \"r2_train\":r2_train, \"r2_val\":r2_val, \"r2_test\":r2_test}, ignore_index=True)\n",
        "\n",
        "df.to_csv(\"elasticnet_tests.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Elasticnet testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 1000\n",
        "alfa = 0.01\n",
        "p_degree = 6\n",
        "lambd = 5\n",
        "regularizer = \"elasticnet\"\n",
        "ratio = 0.9\n",
        "\n",
        "W, loss_train, loss_val, X_test_pow, r2_train, r2_val = train_output_print(X_train, Y_train, X_validation, Y_validation, X_test, Y_test, epochs, alfa, p_degree, lambd, regularizer, ratio)\n",
        "\n",
        "loss_test, r2_test = test_output_print(X_test_pow, W, Y_test, lambd, regularizer, ratio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prueba hiperparámetros sin regularizacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(columns=[\"regularizer\",\"lambd\",\"alfa\",\"epochs\",\"p_degree\",\"ratio\", \"loss_train\", \"loss_val\", \"loss_test\", \"r2_train\", \"r2_val\", \"r2_test\"])\n",
        "for r in [\"None\"]:\n",
        "  for e in [100, 1000, 5000]:\n",
        "    for l in [0.001, 0.01, 0.1, 1, 5]:\n",
        "      for a in [0.1, 0.01, 0.001, 0.0001]:\n",
        "        for p in [1,2,4,6,8,10,20]:\n",
        "          if r == \"elasticnet\":\n",
        "\n",
        "            for ra in [0.3, 0.5, 0.7, 0.9]:\n",
        "              print(\"---\\nregularizer:\",r,\"lambd:\",l,\"alfa:\",a,\"epochs:\",e,\"p_degree:\", p, \"ratio:\", ra)\n",
        "\n",
        "              epochs = e \n",
        "              alfa = a \n",
        "              p_degree = p \n",
        "              lambd = l \n",
        "              regularizer = r \n",
        "              ratio = ra\n",
        "              W, loss_train, loss_val, X_test_pow, r2_train, r2_val = train_output_print(X_train, Y_train, X_validation, Y_validation, X_test, Y_test, epochs, alfa, p_degree, lambd, regularizer, ratio)\n",
        "              loss_test, r2_test = test_output_print(X_test_pow, W, Y_test, lambd, regularizer, ratio)\n",
        "\n",
        "              df = df.append({\"regularizer\":r,\"lambd\":l,\"alfa\":a,\"epochs\":e,\"p_degree\":p,\"ratio\":ra, \"loss_train\":loss_train[-1], \"loss_val\":loss_val[-1], \"loss_test\":loss_test, \"r2_train\":r2_train, \"r2_val\":r2_val, \"r2_test\":r2_test}, ignore_index=True)\n",
        "          else:\n",
        "            print(\"---\\nregularizer:\",r,\"lambd:\",l,\"alfa:\",a,\"epochs:\",e,\"p_degree:\",p)\n",
        "            epochs = e \n",
        "            alfa = a \n",
        "            p_degree = p \n",
        "            lambd = l \n",
        "            regularizer = r \n",
        "            ratio = 0.3 \n",
        "            W, loss_train, loss_val, X_test_pow, r2_train, r2_val = train_output_print(X_train, Y_train, X_validation, Y_validation, X_test, Y_test, epochs, alfa, p_degree, lambd, regularizer, ratio)\n",
        "            loss_test, r2_test = test_output_print(X_test_pow, W, Y_test, lambd, regularizer, ratio)\n",
        "            df = df.append({\"regularizer\":r,\"lambd\":l,\"alfa\":a,\"epochs\":e,\"p_degree\":p,\"ratio\":0, \"loss_train\":loss_train[-1], \"loss_val\":loss_val[-1], \"loss_test\":loss_test, \"r2_train\":r2_train, \"r2_val\":r2_val, \"r2_test\":r2_test}, ignore_index=True)\n",
        "\n",
        "df.to_csv(\"no_regularization_tests.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sin regularizacion testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 5000\n",
        "alfa = 0.1\n",
        "p_degree = 1\n",
        "lambd = 0.001\n",
        "regularizer = \"None\"\n",
        "ratio = 0\n",
        "\n",
        "W, loss_train, loss_val, X_test_pow, r2_train, r2_val = train_output_print(X_train, Y_train, X_validation, Y_validation, X_test, Y_test, epochs, alfa, p_degree, lambd, regularizer, ratio)\n",
        "loss_test, r2_test = test_output_print(X_test_pow, W, Y_test, lambd, regularizer, ratio)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1Vi0-h6DkQlO",
        "SjP0IAdN5idS",
        "SclwheJBirWm",
        "WoVDn02mnd8e",
        "DdcEjGbXjiQa",
        "JO4GEi7OgECf",
        "2D6CZxpvgtNS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "35fddcc3cecf8ec32b60c2df0f0e9dc01da323d81d8d91697dac61b2c06b5614"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
